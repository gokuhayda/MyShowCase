# üìñ Gloss√°rio T√©cnico - CD4ML e MLOps

**√çndice Alfab√©tico:** [A](#a) | [B](#b) | [C](#c) | [D](#d) | [E](#e) | [F](#f) | [G](#g) | [H](#h) | [I](#i) | [K](#k) | [L](#l) | [M](#m) | [O](#o) | [P](#p) | [Q](#r) | [S](#s) | [T](#t) | [V](#v) | [W](#w)

---

## A

### Accuracy (Acur√°cia)
**Defini√ß√£o:** Propor√ß√£o de predi√ß√µes corretas sobre o total de predi√ß√µes.

**F√≥rmula:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Exemplo:**
```
100 predi√ß√µes:
- 85 corretas (TP + TN)
- 15 erradas (FP + FN)
Accuracy = 85/100 = 0.85 (85%)
```

**Quando usar:**
- Classes balanceadas (50/50)

**Quando N√ÉO usar:**
- Classes desbalanceadas (ex: 95/5)
- Nesse caso, modelo que sempre prediz classe majorit√°ria tem 95% accuracy mas √© in√∫til!

**Ver tamb√©m:** [Precision](#precision-precis√£o), [Recall](#recall-sensibilidade)

---

### Artifact
**Defini√ß√£o:** Qualquer arquivo produzido durante o pipeline de ML.

**Exemplos:**
- Modelo treinado (`model.pkl`)
- Dados processados (`features.csv`)
- Plots (`confusion_matrix.png`)
- M√©tricas (`metrics.json`)

**Por que importante:**
- Versionamento (DVC, MLflow)
- Reprodutibilidade
- Auditoria

**No projeto:**
```bash
models/
‚îú‚îÄ‚îÄ model.pkl          # Artifact (modelo)
‚îî‚îÄ‚îÄ metrics.json       # Artifact (m√©tricas)
```

---

### Autostage (DVC)
**Defini√ß√£o:** Configura√ß√£o do DVC que automaticamente adiciona arquivos `.dvc` ao staging do Git.

**Sem autostage:**
```bash
dvc add data.csv
git add data.csv.dvc    # ‚Üê Voc√™ precisa lembrar!
git commit -m "Add data"
```

**Com autostage:**
```bash
dvc config core.autostage true
dvc add data.csv        # J√° adiciona ao git automaticamente!
git commit -m "Add data"
```

**Por que usar:**
- Economiza tempo
- Evita esquecer de versionar ponteiros DVC
- Workflow mais fluido

**Ativar:**
```bash
dvc config core.autostage true
```

**Ver tamb√©m:** [DVC](#dvc-data-version-control)

---

## B

### Batch Prediction
**Defini√ß√£o:** Fazer predi√ß√µes para m√∫ltiplas amostras de uma vez (em lote).

**Compara√ß√£o:**

| Tipo | Quando | Lat√™ncia | Uso |
|------|--------|----------|-----|
| **Single** | 1 amostra por vez | ~1ms | APIs real-time |
| **Batch** | 1000+ amostras juntas | ~100ms total | Relat√≥rios offline |

**Exemplo:**
```python
# Single prediction
prediction = model.predict(sample)  # 1ms

# Batch prediction (mais eficiente!)
predictions = model.predict(batch_1000)  # 50ms total
# = 0.05ms por amostra (20x mais r√°pido!)
```

**Por que usar:**
- Efici√™ncia: Overhead fixo amortizado
- Throughput maior
- Melhor uso de GPU/CPU

**Ver tamb√©m:** [Lat√™ncia](#lat√™ncia)

---

### Blue-Green Deployment
**Defini√ß√£o:** Estrat√©gia de deploy com dois ambientes id√™nticos (Blue=atual, Green=novo).

**Como funciona:**
```
Antes do deploy:
Load Balancer ‚Üí Blue (v1) ‚Üê 100% tr√°fego
                Green (v2) ‚Üê 0% tr√°fego (idle)

Durante deploy:
Load Balancer ‚Üí switch instant√¢neo

Ap√≥s deploy:
Load Balancer ‚Üí Blue (v1) ‚Üê 0% tr√°fego (backup)
                Green (v2) ‚Üê 100% tr√°fego
```

**Vantagens:**
- ‚úÖ Rollback instant√¢neo (< 1 segundo)
- ‚úÖ Zero downtime
- ‚úÖ Test√°vel antes do switch

**Desvantagens:**
- ‚ùå 2x infraestrutura (custo)
- ‚ùå Complexidade operacional

**Quando usar:**
- SLA alto (99.99%+)
- Custo de downtime > custo de infra duplicada

**Ver tamb√©m:** [Canary Deployment](#canary-deployment), [Shadow Mode](#shadow-mode)

---

## C

### Canary Deployment
**Defini√ß√£o:** Estrat√©gia de deploy gradual, liberando nova vers√£o para pequena % de usu√°rios primeiro.

**Analogia:** "Can√°rio na mina" - soltavam can√°rio para testar se o ar estava seguro antes dos mineiros entrarem.

**Progress√£o t√≠pica:**
```
Hora 0:  95% ‚Üí v1 (old)
          5% ‚Üí v2 (canary) üê§

Hora 1:  90% ‚Üí v1
         10% ‚Üí v2  (se m√©tricas OK)

Hora 2:  50% ‚Üí v1
         50% ‚Üí v2  (se ainda OK)

Hora 3:   0% ‚Üí v1
        100% ‚Üí v2  (promover!)
```

**Implementa√ß√£o:**
```python
def get_model(user_id):
    # Hash garante que mesmo usu√°rio sempre v√™ mesma vers√£o
    if hash(user_id) % 100 < 5:  # 5% dos usu√°rios
        return model_v2  # Canary
    else:
        return model_v1  # Stable
```

**Por que funciona:**
- Hash deterministico: mesmo usu√°rio = mesma vers√£o
- F√°cil aumentar %: mude `< 5` para `< 10`, `< 50`, etc
- Minimiza blast radius (raio de impacto)

**M√©tricas a monitorar:**
- Error rate (taxa de erro)
- Latency (p50, p95, p99)
- Business metrics (convers√£o, receita)

**Decis√£o:**
```
Se canary_error_rate > stable_error_rate + threshold:
    ‚Üí ROLLBACK! (canary tem problema)
Sen√£o:
    ‚Üí Aumentar % gradualmente
```

**Vantagens:**
- ‚úÖ Risco minimizado (s√≥ 5% afetados se der ruim)
- ‚úÖ Valida√ß√£o com tr√°fego real
- ‚úÖ Rollback f√°cil (stop em qualquer %)

**Desvantagens:**
- ‚ùå Complexidade (gerenciar 2 vers√µes simultaneamente)
- ‚ùå Lento (pode levar horas/dias)

**Ver tamb√©m:** [Blue-Green](#blue-green-deployment), [A/B Testing](#ab-testing)

---

### CD (Continuous Delivery/Deployment)
**Defini√ß√£o:** Pr√°tica de fazer deploy de c√≥digo automaticamente ap√≥s passar em testes.

**Continuous Delivery vs Deployment:**

| Aspecto | Delivery | Deployment |
|---------|----------|------------|
| **Automa√ß√£o** | At√© staging | At√© produ√ß√£o |
| **Aprova√ß√£o humana** | Necess√°ria para prod | Totalmente autom√°tica |
| **Deploy frequ√™ncia** | Sob demanda | A cada commit |

**Exemplo Continuous Delivery:**
```
git push ‚Üí tests pass ‚Üí build ‚Üí deploy to staging
                                      ‚Üì
                            [Human approval needed]
                                      ‚Üì
                            Deploy to production
```

**Exemplo Continuous Deployment:**
```
git push ‚Üí tests pass ‚Üí build ‚Üí deploy to staging
                              ‚Üí deploy to production ‚úÖ
                              (TUDO autom√°tico!)
```

**Benef√≠cios:**
- ‚úÖ Deploy frequente (m√∫ltiplos por dia)
- ‚úÖ Feedback r√°pido
- ‚úÖ Bugs pequenos (f√°cil debugar)
- ‚úÖ Rollback f√°cil

**Ver tamb√©m:** [CI](#ci-continuous-integration), [CD4ML](#cd4ml-continuous-delivery-for-machine-learning)

---

### CD4ML (Continuous Delivery for Machine Learning)
**Defini√ß√£o:** Aplica√ß√£o de pr√°ticas de CI/CD ao Machine Learning, adaptando para desafios √∫nicos de ML.

**Por que ML √© diferente de software tradicional?**

| Aspecto | Software | ML |
|---------|----------|-----|
| **Input** | C√≥digo | C√≥digo + **DADOS** |
| **Output** | Determin√≠stico | **Probabil√≠stico** |
| **Testes** | `assert x == 5` | `assert accuracy > 0.9` |
| **Degrada√ß√£o** | Bug no c√≥digo | **Data drift** |
| **Reprodu√ß√£o** | `git checkout` | `git + DVC + ambiente` |

**Os 4 Pilares do CD4ML:**
```
1. VERSIONAMENTO
   ‚îú‚îÄ C√≥digo: Git
   ‚îú‚îÄ Dados: DVC ‚Üê Novo!
   ‚îú‚îÄ Modelos: MLflow ‚Üê Novo!
   ‚îî‚îÄ Ambiente: Docker

2. AUTOMA√á√ÉO
   ‚îú‚îÄ ETL autom√°tico
   ‚îú‚îÄ Treino autom√°tico
   ‚îú‚îÄ Valida√ß√£o autom√°tica
   ‚îî‚îÄ Deploy autom√°tico

3. TESTES (3 camadas)
   ‚îú‚îÄ Dados: Schema, ranges, drift
   ‚îú‚îÄ Modelo: Accuracy > threshold
   ‚îî‚îÄ Infer√™ncia: Latency < 100ms

4. MONITORAMENTO
   ‚îú‚îÄ Data drift (P(X) mudou?)
   ‚îú‚îÄ Model drift (P(≈∂) mudou?)
   ‚îî‚îÄ Concept drift (P(Y|X) mudou?)
```

**Exemplo de pipeline CD4ML:**
```
git push
    ‚Üì
[CI/CD Pipeline]
    ‚îú‚îÄ Test data quality ‚úÖ
    ‚îú‚îÄ Train model
    ‚îú‚îÄ Test model metrics ‚úÖ
    ‚îú‚îÄ Test inference ‚úÖ
    ‚îî‚îÄ Deploy (se tudo passar)
```

**Refer√™ncia:** [Martin Fowler - CD4ML](https://martinfowler.com/articles/cd4ml.html)

**Ver tamb√©m:** [Drift](#drift), [DVC](#dvc-data-version-control), [MLflow](#mlflow)

---

### CI (Continuous Integration)
**Defini√ß√£o:** Pr√°tica de integrar c√≥digo frequentemente (v√°rias vezes ao dia), com testes autom√°ticos a cada integra√ß√£o.

**Problema que resolve:**

**‚ùå Sem CI (Integration Hell):**
```
Semana toda:
Dev A codifica na branch dele
Dev B codifica na branch dele
Dev C codifica na branch dele

Sexta √† tarde:
"Vamos juntar tudo!" ‚Üí CONFLITOS! üò±
Fim de semana inteiro debugando...
```

**‚úÖ Com CI:**
```
Dev A: git push ‚Üí testes rodam (5 min) ‚Üí ‚úÖ passou
Dev B: git push ‚Üí testes rodam (5 min) ‚Üí ‚ùå quebrou!
       Dev B corrige IMEDIATAMENTE (problema ainda fresco)
Dev C: git push ‚Üí testes rodam (5 min) ‚Üí ‚úÖ passou
```

**Exemplo GitHub Actions:**
```yaml
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pip install -r requirements.txt
      - run: pytest tests/ -v  # Autom√°tico!
```

**Benef√≠cios:**
- ‚úÖ Problemas detectados em minutos (n√£o em dias)
- ‚úÖ C√≥digo sempre em estado "funcionando"
- ‚úÖ Menos conflitos de merge
- ‚úÖ Feedback r√°pido

**Regras de ouro do CI:**
1. Commitar frequentemente (v√°rias vezes ao dia)
2. Testes devem ser r√°pidos (< 10 min)
3. Se testes quebram, consertar √© prioridade #1

**Ver tamb√©m:** [CD](#cd-continuous-deliverydeployment), [CI/CD](#cicd)

---

### CI/CD
**Defini√ß√£o:** Combina√ß√£o de Continuous Integration + Continuous Delivery/Deployment.

**Fluxo completo:**
```
Desenvolvedor:
‚îú‚îÄ Escreve c√≥digo
‚îú‚îÄ Escreve testes
‚îú‚îÄ git commit
‚îî‚îÄ git push
      ‚Üì
CI (Continuous Integration):
‚îú‚îÄ Detecta push automaticamente
‚îú‚îÄ Instala depend√™ncias
‚îú‚îÄ Roda testes
‚îî‚îÄ ‚úÖ Passou? ‚Üí pr√≥ximo passo
   ‚ùå Falhou? ‚Üí notifica e para
      ‚Üì
CD (Continuous Delivery):
‚îú‚îÄ Build da aplica√ß√£o
‚îú‚îÄ Deploy para staging
‚îú‚îÄ Testes de integra√ß√£o
‚îî‚îÄ Aprova√ß√£o (manual ou autom√°tica)
      ‚Üì
CD (Continuous Deployment):
‚îú‚îÄ Deploy para produ√ß√£o
‚îú‚îÄ Smoke tests
‚îî‚îÄ Monitoramento
```

**Ferramentas populares:**
- GitHub Actions (usado neste projeto)
- GitLab CI
- Jenkins
- CircleCI
- Travis CI

**Ver tamb√©m:** [CI](#ci-continuous-integration), [CD](#cd-continuous-deliverydeployment)

---

### Class Weight (Peso de Classe)
**Defini√ß√£o:** T√©cnica para balancear classes desbalanceadas atribuindo pesos diferentes a cada classe.

**Problema:**
```
Dataset desbalanceado:
Class 0 (bad wine):  100 amostras
Class 1 (good wine):  10 amostras

Sem balanceamento:
Modelo aprende: "Sempre prediz 0"
Accuracy: 90% üéâ ... mas √© IN√öTIL! (ignora classe 1)
```

**Solu√ß√£o com class_weight:**
```python
# Scikit-learn calcula pesos automaticamente
model = RandomForestClassifier(class_weight='balanced')

# Ou manualmente:
# class_weight = {0: 1.0, 1: 10.0}  # Classe 1 vale 10x mais
```

**Como funciona:**
```
Perda (loss) sem peso:
loss = erro_classe_0 + erro_classe_1

Perda COM peso balanceado:
loss = (1.0 * erro_classe_0) + (10.0 * erro_classe_1)
                                  ‚Üë
                    Modelo "se importa" 10x mais com classe 1!
```

**F√≥rmula de 'balanced':**
```
weight_class_i = n_samples / (n_classes * n_samples_class_i)

Exemplo:
Total: 110 amostras, 2 classes
Class 0: 100 amostras ‚Üí weight = 110 / (2 * 100) = 0.55
Class 1:  10 amostras ‚Üí weight = 110 / (2 * 10)  = 5.50
```

**Quando usar:**
- Classes desbalanceadas (ex: 90/10, 95/5)
- Classe minorit√°ria √© importante (ex: detec√ß√£o de fraude)

**Alternativas:**
- Oversampling (SMOTE)
- Undersampling
- Threshold adjustment

**Ver tamb√©m:** [Precision](#precision-precis√£o), [Recall](#recall-sensibilidade)

---

### Concept Drift
**Defini√ß√£o:** Mudan√ßa na rela√ß√£o entre features (X) e target (Y), ou seja, P(Y|X) muda.

**Tipo mais grave de drift!**

**Exemplo real - COVID:**
```
ANTES (2019):
Features: Renda alta + Score 750
Target: Baixo risco de cr√©dito ‚úÖ
P(inadimplente | renda_alta) = 5%

DURANTE (2020 - COVID):
Features: Renda alta + Score 750  (MESMAS features!)
Target: ALTO risco de cr√©dito ‚ùå
P(inadimplente | renda_alta) = 30%  ‚Üê MUDOU!

Causa: Economia mudou, rela√ß√£o mudou!
```

**Outro exemplo - Spam:**
```
ANTES:
"Compre Viagra" ‚Üí Spam (100%)

DEPOIS:
"Compre Viagra" ‚Üí Legit (farm√°cia real)
                   ‚Üë
          Contexto mudou!
```

**Como detectar:**
```python
# Monitorar acur√°cia ao longo do tempo
if accuracy_semana_atual < accuracy_baseline - threshold:
    print("‚ö†Ô∏è Poss√≠vel concept drift!")
```

**Solu√ß√µes:**
1. **Feature engineering** (adicionar contexto)
```python
   # Adicionar features temporais
   df['is_pandemic'] = (df['date'] >= '2020-03-01')
```

2. **Janela temporal menor**
```python
   # Treinar apenas com √∫ltimos 3 meses
   train_data = df[df['date'] > '2024-10-01']
```

3. **Online learning**
```python
   # Retreinar continuamente
   model.partial_fit(new_data_batch)
```

4. **Retreinar periodicamente**
```python
   # Scheduled retraining (ex: mensal)
   if datetime.now().day == 1:
       retrain_model()
```

**Ver tamb√©m:** [Data Drift](#data-drift), [Model Drift](#model-drift)

---

### Cross-Validation (Valida√ß√£o Cruzada)
**Defini√ß√£o:** T√©cnica para estimar performance do modelo de forma mais robusta, dividindo dados em K folds.

**Por que usar?**
```
‚ùå Treino/teste simples:
Train (80%) ‚Üí Test (20%)
Problema: E se test set for "f√°cil" ou "dif√≠cil" por sorte?
```

**‚úÖ 5-Fold Cross-Validation:**
```
Fold 1: [Train][Train][Train][Train][TEST]
Fold 2: [Train][Train][Train][TEST][Train]
Fold 3: [Train][Train][TEST][Train][Train]
Fold 4: [Train][TEST][Train][Train][Train]
Fold 5: [TEST][Train][Train][Train][Train]

Resultado final: M√©dia dos 5 testes ¬± desvio padr√£o
```

**Exemplo:**
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
# scores = [0.85, 0.87, 0.84, 0.86, 0.88]

print(f"Accuracy: {scores.mean():.2f} ¬± {scores.std():.2f}")
# Accuracy: 0.86 ¬± 0.01
```

**Interpreta√ß√£o:**
```
Accuracy: 0.86 ¬± 0.01
          ‚Üë      ‚Üë
       m√©dia   varia√ß√£o

Std baixo (< 0.05): Modelo est√°vel ‚úÖ
Std alto (> 0.10):  Modelo inst√°vel ‚ùå (sens√≠vel ao split)
```

**Variantes:**
- **K-Fold**: K divis√µes iguais
- **Stratified K-Fold**: Mant√©m propor√ß√£o de classes (SEMPRE use para classifica√ß√£o!)
- **Leave-One-Out**: K = n_samples (muito lento)
- **Time Series Split**: Respeita ordem temporal

**Ver tamb√©m:** [Overfitting](#overfitting)

---

## D

### Data Drift
**Defini√ß√£o:** Mudan√ßa na distribui√ß√£o das features (input), ou seja, P(X) muda, mas P(Y|X) permanece.

**Exemplo:**
```
E-commerce:

ANTES (2019):
Usu√°rios: 70% formal, 30% casual

DEPOIS (Pandemia 2020):
Usu√°rios: 10% formal, 90% casual ‚Üê P(X) mudou!

Problema:
Modelo foi treinado com 70% formal
Agora v√™ 90% casual ‚Üí Performa mal!
```

**Como detectar:**
```python
from scipy.stats import ks_2samp

# Comparar distribui√ß√£o de 'age' entre treino e produ√ß√£o
stat, p_value = ks_2samp(train_data['age'], prod_data['age'])

if p_value < 0.05:
    print("üö® Data drift detectado na feature 'age'!")
```

**Interpreta√ß√£o do p-value:**
```
p_value = 0.8  (alto)  ‚Üí Distribui√ß√µes parecidas ‚úÖ
p_value = 0.001 (baixo) ‚Üí Distribui√ß√µes DIFERENTES üö®
```

**Solu√ß√£o:**
```python
# Retreinar com dados recentes
new_train_data = get_recent_data(last_6_months=True)
model.fit(new_train_data)
```

**Ferramentas:**
- **Evidently**: Dashboard de drift
- **KS Test**: Teste estat√≠stico (usado acima)
- **PSI** (Population Stability Index)
- **Chi-squared test**: Para features categ√≥ricas

**Ver tamb√©m:** [Model Drift](#model-drift), [Concept Drift](#concept-drift), [KS Test](#ks-test-kolmogorov-smirnov)

---

### DVC (Data Version Control)
**Defini√ß√£o:** "Git para dados" - ferramenta para versionar datasets, modelos e pipelines de ML.

**Por que DVC existe?**

**‚ùå Problema com Git:**
```bash
# Dataset de 10 GB
git add data/train.csv  # ‚Üê Git guarda TUDO no repo
git commit

# 10 vers√µes = 100 GB no repo Git üò±
# git clone demora HORAS
# GitHub rejeita (limite: 100 MB por arquivo)
```

**‚úÖ Solu√ß√£o com DVC:**
```bash
# DVC guarda apenas ponteiro (~1 KB)
dvc add data/train.csv

# Cria:
# 1. data/train.csv.dvc (ponteiro, vai pro Git)
# 2. Move arquivo para .dvc/cache
# 3. Adiciona train.csv ao .gitignore

git add data/train.csv.dvc
git commit -m "Add training data"

# Repo Git: apenas ponteiro (1 KB)
# Dados reais: em S3/GCS (10 GB)
```

**Como funciona:**
```
Git armazena:               DVC armazena:
‚îú‚îÄ C√≥digo (.py)            ‚îú‚îÄ Dados (.csv)
‚îú‚îÄ Configs (.yaml)         ‚îú‚îÄ Modelos (.pkl)
‚îî‚îÄ Ponteiros (.dvc)        ‚îî‚îÄ Artifacts grandes
   (~1 KB cada)               (GB/TB)
```

**Comandos essenciais:**
```bash
# Inicializar
dvc init --subdir  # Em subdiret√≥rio de um repo Git

# Adicionar dados
dvc add data/train.csv

# Configurar remote (S3, GCS, Azure, etc)
dvc remote add -d myremote s3://my-bucket/dvc-storage

# Push/Pull
dvc push  # Upload para remote
dvc pull  # Download do remote

# Reproduzir pipeline
dvc repro

# Ver DAG
dvc dag
```

**Reprodutibilidade:**
```bash
# Experimento de 6 meses atr√°s
git checkout <commit-6-meses>
dvc checkout

# Agora voc√™ tem:
# ‚úÖ C√≥digo exato (Git)
# ‚úÖ Dados exatos (DVC)
# ‚úÖ Pode retreinar e obter MESMO resultado!
```

**Ver tamb√©m:** [Hash](#hash-md5), [Autostage](#autostage-dvc), [DVC Pipeline](#dvc-pipeline)

---

### DVC Pipeline
**Defini√ß√£o:** Grafo de depend√™ncias (DAG) que define etapas do pipeline de ML e suas rela√ß√µes.

**Definido em `dvc.yaml`:**
```yaml
stages:
  prepare_data:
    cmd: python src/data/make_dataset.py
    deps:
      - data/raw/wine_quality.csv
      - src/data/make_dataset.py
    outs:
      - data/processed/wine_features.csv
  
  train:
    cmd: python src/models/train.py
    deps:
      - data/processed/wine_features.csv
      - src/models/train.py
      - params.yaml
    params:
      - model
      - data
    outs:
      - models/model.pkl
    metrics:
      - models/metrics.json:
          cache: false
```

**Visualizar DAG:**
```bash
dvc dag

# Output:
#   +---------------+
#   | prepare_data  |
#   +---------------+
#           *
#           *
#           *
#      +-------+
#      | train |
#      +-------+
```

**Executar pipeline:**
```bash
# Reproduzir TUDO
dvc repro

# DVC √© inteligente:
# - Se nada mudou ‚Üí "Everything is up to date"
# - Se params.yaml mudou ‚Üí Re-treina (mas n√£o refaz ETL)
# - Se dados mudaram ‚Üí Refaz TUDO
```

**Benef√≠cios:**
- ‚úÖ Cache inteligente (n√£o refaz o que n√£o precisa)
- ‚úÖ Reprodutibilidade (DAG versionado no Git)
- ‚úÖ Paraleliza√ß√£o autom√°tica
- ‚úÖ Rastreabilidade (quem depende de quem)

**Ver tamb√©m:** [DVC](#dvc-data-version-control)

---

## E

### ETL (Extract, Transform, Load)
**Defini√ß√£o:** Processo de extrair dados da fonte, transformar (limpar, processar) e carregar para destino.

**No contexto de ML:**
```
EXTRACT:
‚îî‚îÄ Baixar dataset (API, CSV, DB)

TRANSFORM:
‚îú‚îÄ Limpeza (remover NaN, duplicatas)
‚îú‚îÄ Valida√ß√£o (schema, ranges)
‚îú‚îÄ Feature engineering
‚îî‚îÄ Normaliza√ß√£o/encoding

LOAD:
‚îî‚îÄ Salvar features processadas
```

**Exemplo neste projeto:**
```python
# src/data/make_dataset.py

def main():
    # EXTRACT
    df_raw = load_raw_data("data/raw/wine_quality.csv")
    
    # TRANSFORM
    df_processed = create_features(df_raw)
    
    # LOAD
    save_processed_data(df_processed, "data/processed/wine_features.csv")
```

**Boas pr√°ticas:**
- ‚úÖ Validar na entrada (Pandera schemas)
- ‚úÖ Idempotente (rodar 2x = mesmo resultado)
- ‚úÖ Logado (saber o que aconteceu)
- ‚úÖ Testado (testes de dados)

**Ver tamb√©m:** [Feature Engineering](#feature-engineering)

---

## F

### F1-Score
**Defini√ß√£o:** M√©dia harm√¥nica entre Precision e Recall. Balanceia ambas as m√©tricas.

**F√≥rmula:**
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

**Por que m√©dia harm√¥nica (n√£o aritm√©tica)?**
```
Caso patol√≥gico:
Precision = 1.0 (perfeita!)
Recall    = 0.01 (p√©ssimo!)

M√©dia aritm√©tica: (1.0 + 0.01) / 2 = 0.505  ‚Üê ENGANOSO!
M√©dia harm√¥nica:  2 * (1.0 * 0.01) / 1.01 = 0.019  ‚Üê HONESTO!
                  ‚Üë
          Penaliza valores muito diferentes
```

**Exemplo:**
```
Confusion Matrix:
       Pred 0  Pred 1
True 0   120     30     (TN=120, FP=30)
True 1    15    155     (FN=15, TP=155)

Precision = 155 / (155 + 30) = 0.838
Recall    = 155 / (155 + 15) = 0.912

F1 = 2 * (0.838 * 0.912) / (0.838 + 0.912)
   = 2 * 0.764 / 1.75
   = 0.873
```

**Quando usar:**
- Classes desbalanceadas
- Voc√™ se importa IGUALMENTE com Precision e Recall
- M√©trica √∫nica para comparar modelos

**Variantes:**
- **F2-Score**: D√° 2x mais peso ao Recall
```
  F2 = 5 * P * R / (4*P + R)
```
- **F0.5-Score**: D√° 2x mais peso ao Precision

**Ver tamb√©m:** [Precision](#precision-precis√£o), [Recall](#recall-sensibilidade)

---

### Feature Engineering
**Defini√ß√£o:** Processo de criar novas features (vari√°veis) a partir das existentes para melhorar performance do modelo.

**Tipos:**

**1. Transforma√ß√µes matem√°ticas:**
```python
# Log transform (reduzir skewness)
df['log_price'] = np.log1p(df['price'])

# Polynomial features
df['age_squared'] = df['age'] ** 2

# Raz√µes
df['debt_to_income'] = df['debt'] / df['income']
```

**2. Binning (discretiza√ß√£o):**
```python
# Idade cont√≠nua ‚Üí Grupos
df['age_group'] = pd.cut(df['age'], 
                          bins=[0, 18, 30, 50, 100],
                          labels=['teen', 'young', 'adult', 'senior'])
```

**3. Encoding categ√≥ricas:**
```python
# One-hot encoding
pd.get_dummies(df['city'])

# Label encoding
df['size_encoded'] = df['size'].map({'S': 1, 'M': 2, 'L': 3})
```

**4. Features temporais:**
```python
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6])
df['hour'] = df['timestamp'].dt.hour
```

**5. Agrega√ß√µes:**
```python
# M√©dia por grupo
df['avg_price_by_category'] = df.groupby('category')['price'].transform('mean')
```

**6. Intera√ß√µes:**
```python
# Multiplicar features
df['feature_interaction'] = df['feature_A'] * df['feature_B']
```

**No nosso projeto:**
```python
# src/data/make_dataset.py

def create_features(df):
    # Target engineering
    df['quality_binary'] = (df['quality'] >= 6).astype(int)
    
    # Poss√≠veis melhorias:
    # df['acidity_ratio'] = df['fixed_acidity'] / df['volatile_acidity']
    # df['sugar_to_alcohol'] = df['residual_sugar'] / df['alcohol']
    
    return df
```

**Dica:** Come√ßar simples, adicionar complexidade conforme necess√°rio.

**Ver tamb√©m:** [ETL](#etl-extract-transform-load)

---

### Feature Store
**Defini√ß√£o:** Sistema centralizado para armazenar, servir e compartilhar features entre treino e infer√™ncia.

**Problema que resolve (Training-Serving Skew):**
```
‚ùå SEM Feature Store:

TREINO (Data Scientist):
def calculate_features(df):
    return df['price'].rolling(7).mean()  # Python/Pandas

PRODU√á√ÉO (Engineer):
def calculate_features(data):
    // Reimplementado em Java
    return rollingAverage(data.price, 7);  // BUG: implementa√ß√£o diferente!

Resultado:
Treino: 95% accuracy
Produ√ß√£o: 70% accuracy üò± (WTF?!)
```

**‚úÖ COM Feature Store:**
```
Feature Store (Feast, Tecton, etc)
          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           ‚îÇ
  Treino    Infer√™ncia
(MESMAS    (MESMAS
features!)  features!)
```

**Exemplo (Feast):**
```python
from feast import FeatureStore

# Definir feature UMA VEZ
@feast.feature_view(...)
def user_features():
    return DataFrame([
        Feature("age", ValueType.INT64),
        Feature("activity_30d", ValueType.INT64)
    ])

# TREINO
training_df = store.get_historical_features(
    entity_df=entities,
    features=["user_features:age", "user_features:activity_30d"]
)

# INFER√äNCIA (MESMAS features!)
online_features = store.get_online_features(
    features=["user_features:age", "user_features:activity_30d"],
    entity_rows=[{"user_id": 123}]
)
```

**Benef√≠cios:**
- ‚úÖ **Consist√™ncia**: Treino = Infer√™ncia (sem skew)
- ‚úÖ **Reuso**: Times compartilham features
- ‚úÖ **Performance**: Features pr√©-computadas
- ‚úÖ **Governan√ßa**: Versionamento, lineage

**Ferramentas:**
- Feast (open source)
- Tecton (enterprise)
- Databricks Feature Store
- AWS SageMaker Feature Store

**Ver tamb√©m:** [Training-Serving Skew](#training-serving-skew)

---

## G

### Git
**Defini√ß√£o:** Sistema de controle de vers√£o distribu√≠do para c√≥digo.

**Comandos b√°sicos:**
```bash
# Clonar reposit√≥rio
git clone <url>

# Status
git status

# Adicionar mudan√ßas
git add arquivo.py
git add .  # Todos os arquivos

# Commit
git commit -m "Mensagem descritiva"

# Push (enviar para remoto)
git push origin main

# Pull (baixar do remoto)
git pull origin main

# Branches
git checkout -b feature/nova-feature
git merge feature/nova-feature

# Ver hist√≥rico
git log --oneline

# Voltar no tempo
git checkout <commit-hash>
```

**Boas pr√°ticas de commit:**
```
feat: Add new feature
fix: Fix bug in model training
docs: Update README
test: Add unit tests for ETL
refactor: Simplify data pipeline
chore: Update dependencies
```

**Ver tamb√©m:** [DVC](#dvc-data-version-control), [GitHub Actions](#github-actions)

---

### GitHub Actions
**Defini√ß√£o:** Plataforma de CI/CD integrada ao GitHub que executa workflows automaticamente.

**Exemplo b√°sico:**
```yaml
# .github/workflows/test.yaml

name: Run Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run tests
        run: pytest tests/ -v
```

**Triggers:**
- `push`: A cada push
- `pull_request`: A cada PR
- `schedule`: Cron (ex: di√°rio √†s 2am)
- `workflow_dispatch`: Manual

**Ver tamb√©m:** [CI/CD](#cicd)

---

## H

### Hash (MD5)
**Defini√ß√£o:** Fun√ß√£o que transforma dados de qualquer tamanho em uma "impress√£o digital" fixa (32 caracteres no MD5).

**Analogia:** Impress√£o digital humana
```
Pessoa A ‚Üí Impress√£o digital: ‚àû‚àû‚àû‚àû‚àû (√∫nica)
Pessoa B ‚Üí Impress√£o digital: ‚âà‚âà‚âà‚âà‚âà (√∫nica)
```

**Hash de arquivos:**
```
Arquivo X ‚Üí Hash: a3e4f5c6d7e8... (√∫nico)
Arquivo Y ‚Üí Hash: 7b8c9d0e1f2a... (√∫nico)
```

**Propriedades m√°gicas:**

**1. Determin√≠stico:**
```
arquivo.csv ‚Üí md5sum ‚Üí a3e4f5c6...
arquivo.csv ‚Üí md5sum ‚Üí a3e4f5c6...  (sempre igual!)
```

**2. Sens√≠vel a mudan√ßas:**
```
"Jo√£o,25" ‚Üí Hash: a3e4f5c6...
"Jo√£o,26" ‚Üí Hash: 7b8c9d0e...  (totalmente diferente!)
      ‚Üë
   Mudou 1 caractere!
```

**3. Tamanho fixo:**
```
1 KB   ‚Üí Hash: a3e4f5c6... (32 chars)
5 GB   ‚Üí Hash: 7b8c9d0e... (32 chars)
```

**Exemplo pr√°tico:**
```bash
# Calcular hash de um arquivo
md5sum data/train.csv
# Output: a3e4f5c6d7e8f9a0b1c2d3e4f5a6b7c8  data/train.csv

# 6 meses depois...
md5sum data/train.csv
# Output: a3e4f5c6d7e8f9a0b1c2d3e4f5a6b7c8

# Hash igual = arquivo N√ÉO mudou! ‚úÖ
```

**Por que DVC usa hash:**
```
‚ùå Comparar arquivos byte por byte:
5 GB √ó 5 GB = MUITO LENTO (minutos)

‚úÖ Comparar hashes:
32 chars = 32 chars? INSTANT√ÇNEO (milissegundos)
```

**Calcular em Python:**
```python
import hashlib

def calcular_hash(arquivo):
    md5 = hashlib.md5()
    
    with open(arquivo, 'rb') as f:
        while chunk := f.read(8192):
            md5.update(chunk)
    
    return md5.hexdigest()

hash_result = calcular_hash('train.csv')
print(hash_result)  # a3e4f5c6d7e8f9a0b1c2d3e4f5a6b7c8
```

**Ver tamb√©m:** [DVC](#dvc-data-version-control)

---

## I

### Inference (Infer√™ncia)
**Defini√ß√£o:** Processo de usar modelo treinado para fazer predi√ß√µes em dados novos.

**Tipos:**

**1. Single prediction (tempo real):**
```python
sample = {'age': 25, 'income': 50000}
prediction = model.predict(sample)
# Lat√™ncia: ~1ms
```

**2. Batch prediction (offline):**
```python
batch = pd.read_csv('new_customers.csv')
predictions = model.predict(batch)
# Lat√™ncia: ~100ms para 1000 samples
```

**Exemplo completo:**
```python
# src/models/predict.py

class WineQualityPredictor:
    def __init__(self):
        self.model = pickle.load(open('models/model.pkl', 'rb'))
    
    def predict(self, X):
        return self.model.predict(X)
    
    def predict_with_confidence(self, X):
        pred = self.predict(X)[0]
        proba = self.model.predict_proba(X)[0]
        
        return {
            'prediction': int(pred),
            'confidence': float(proba[pred]),
            'probabilities': {
                0: float(proba[0]),
                1: float(proba[1])
            }
        }
```

**Testes de infer√™ncia:**
- ‚úÖ Lat√™ncia < threshold
- ‚úÖ Formato correto
- ‚úÖ Determinismo
- ‚úÖ Edge cases

**Ver tamb√©m:** [Lat√™ncia](#lat√™ncia), [Batch Prediction](#batch-prediction)

---

## K

### KS Test (Kolmogorov-Smirnov)
**Defini√ß√£o:** Teste estat√≠stico que compara duas distribui√ß√µes para verificar se s√£o diferentes.

**Uso em ML:** Detectar data drift

**Exemplo:**
```python
from scipy.stats import ks_2samp

# Comparar idades entre treino e produ√ß√£o
stat, p_value = ks_2samp(train_data['age'], prod_data['age'])

if p_value < 0.05:
    print("üö® Drift detectado!")
else:
    print("‚úÖ Distribui√ß√µes parecidas")
```

**Interpreta√ß√£o do p-value:**
```
p_value > 0.05:
"N√£o h√° evid√™ncia de que as distribui√ß√µes sejam diferentes"
Interpreta√ß√£o: Distribui√ß√µes parecem IGUAIS ‚úÖ

p_value < 0.05:
"H√° forte evid√™ncia de que as distribui√ß√µes s√£o diferentes"
Interpreta√ß√£o: Distribui√ß√µes s√£o DIFERENTES üö®
```

**Visualiza√ß√£o:**
```
Treino:    |        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
           |       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
           |      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
           10    20    30    40

Produ√ß√£o:  |                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
           |               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
           |              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
           10    20    30    40    50

KS statistic = M√°xima dist√¢ncia vertical entre curvas
p_value = Probabilidade dessa dist√¢ncia ser "normal"
```

**Ver tamb√©m:** [Data Drift](#data-drift), [P-value](#p-value)

---

## L

### Lat√™ncia
**Defini√ß√£o:** Tempo entre enviar requisi√ß√£o e receber resposta.

**Em ML:** Tempo para fazer uma predi√ß√£o

**Medidas t√≠picas:**

| Cen√°rio | Lat√™ncia | Aceit√°vel? |
|---------|----------|------------|
| **APIs real-time** | < 100ms | ‚úÖ |
| **Batch processing** | < 1s para 100 samples | ‚úÖ |
| **GPU inference** | < 10ms | ‚úÖ |
| **API lenta** | > 1s | ‚ùå (usu√°rio sente) |

**Medindo lat√™ncia:**
```python
import time

start = time.time()
prediction = model.predict(sample)
latency_ms = (time.time() - start) * 1000

print(f"Latency: {latency_ms:.2f}ms")
```

**Percentis (mais importante que m√©dia!):**
```python
# Fazer 1000 predi√ß√µes
latencies = []
for _ in range(1000):
    start = time.time()
    model.predict(sample)
    latencies.append((time.time() - start) * 1000)

# Analisar distribui√ß√£o
p50 = np.percentile(latencies, 50)  # Mediana
p95 = np.percentile(latencies, 95)  # 95% das requests
p99 = np.percentile(latencies, 99)  # 99% das requests

print(f"p50: {p50:.2f}ms")  # Metade < isso
print(f"p95: {p95:.2f}ms")  # 95% < isso
print(f"p99: {p99:.2f}ms")  # 99% < isso

# Exemplo:
# p50: 5ms   ‚Üê Metade √© super r√°pido
# p95: 50ms  ‚Üê 95% OK
# p99: 500ms ‚Üê 1% muito lento! (investigar)
```

**Por que p99 importa:**
```
1 milh√£o de requests/dia
1% com 500ms de lat√™ncia
= 10.000 usu√°rios frustrados! üò†
```

**Ver tamb√©m:** [Inference](#inference-infer√™ncia)

---

## M

### Makefile
**Defini√ß√£o:** Arquivo que define comandos √∫teis (atalhos) para tarefas comuns do projeto.

**Exemplo:**
```makefile
# Makefile

.PHONY: test train clean

# Rodar testes
test:
	pytest src/tests/ -v

# Treinar modelo
train:
	python src/models/train.py

# Limpar cache
clean:
	rm -rf __pycache__
	rm -rf .pytest_cache
	find . -name "*.pyc" -delete

# Docker build
docker-build:
	docker build -t cd4ml-wine .

# Docker run
docker-run:
	docker run --rm cd4ml-wine
```

**Uso:**
```bash
make test         # Ao inv√©s de: pytest src/tests/ -v
make train        # Ao inv√©s de: python src/models/train.py
make docker-build # Ao inv√©s de: docker build -t cd4ml-wine .
```

**Benef√≠cios:**
- ‚úÖ Comandos padronizados
- ‚úÖ Documenta√ß√£o viva
- ‚úÖ Onboarding mais f√°cil
- ‚úÖ Menos erros de digita√ß√£o

---

### MLflow
**Defini√ß√£o:** Plataforma open-source para gerenciar o ciclo de vida de ML: tracking, projetos, modelos e deployment.

**4 Componentes:**

**1. MLflow Tracking** (mais usado):
```python
import mlflow

# Logar experimento
with mlflow.start_run():
    # Par√¢metros
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 10)
    
    # M√©tricas
    mlflow.log_metric("accuracy", 0.87)
    mlflow.log_metric("f1", 0.85)
    
    # Artifacts
    mlflow.sklearn.log_model(model, "model")
    mlflow.log_artifact("plots/confusion_matrix.png")
    
    # Tags
    mlflow.set_tag("author", "eric@company.com")
```

**2. MLflow Projects:**
Definir ambiente reprodut√≠vel (MLproject file)

**3. MLflow Models:**
Formato padr√£o para empacotar modelos

**4. MLflow Model Registry:**
Gerenciar modelos em produ√ß√£o (Staging ‚Üí Production)

**Ver experimentos:**
```bash
# Iniciar UI
mlflow ui

# Abrir navegador:
# http://localhost:5000
```

**Benef√≠cios:**
- ‚úÖ Rastreabilidade (quem, quando, o qu√™)
- ‚úÖ Compara√ß√£o de experimentos
- ‚úÖ Reprodutibilidade
- ‚úÖ Compliance ready

**Ver tamb√©m:** [Experiment Tracking](#experiment-tracking)

---

### Model Drift
**Defini√ß√£o:** Mudan√ßa na distribui√ß√£o das predi√ß√µes do modelo, ou seja, P(≈∂) muda.

**Exemplo:**
```
Spam detector:

TREINO:
Predi√ß√µes: 5% spam, 95% ham

PRODU√á√ÉO (depois de semanas):
Predi√ß√µes: 30% spam, 70% ham  ‚Üê P(≈∂) mudou!

Causas poss√≠veis:
1. Data drift (input mudou)
2. Bug no c√≥digo
3. Adversarial attack (spammers se adaptaram)
```

**Como detectar:**
```python
# Monitorar distribui√ß√£o de predi√ß√µes
train_pred_dist = model.predict(train_data).mean()  # 0.50
prod_pred_dist = model.predict(prod_data).mean()    # 0.75

if abs(prod_pred_dist - train_pred_dist) > 0.10:
    print("‚ö†Ô∏è Model drift detectado!")
```

**Solu√ß√µes:**
1. Investigar causa raiz (data drift? bug?)
2. Retreinar se necess√°rio
3. A/B test (novo vs antigo)

**Ver tamb√©m:** [Data Drift](#data-drift), [Concept Drift](#concept-drift)

---

## O

### Overfitting
**Defini√ß√£o:** Modelo "decora" os dados de treino ao inv√©s de aprender padr√µes generaliz√°veis.

**Sintoma:**
```
Train accuracy: 99% üéâ
Test accuracy:  60% üò±

Gap = 39% ‚Üê OVERFITTING!
```

**Analogia:** Estudante que decora respostas da prova passada mas n√£o entende a mat√©ria.

**Causas:**
- Modelo muito complexo (√°rvore profunda demais)
- Poucos dados
- Features ruins (data leakage)

**Solu√ß√µes:**

**1. Regulariza√ß√£o:**
```python
# L1 (Lasso) ou L2 (Ridge)
model = RandomForestClassifier(
    max_depth=5,           # Limitar complexidade
    min_samples_split=10,  # Exigir mais samples
)
```

**2. Mais dados:**
```python
# Data augmentation, oversampling, etc
```

**3. Cross-validation:**
```python
# Avaliar em m√∫ltiplos folds
scores = cross_val_score(model, X, y, cv=5)
```

**4. Early stopping:**
```python
# Parar treino quando valida√ß√£o n√£o melhora
```

**Ver tamb√©m:** [Cross-Validation](#cross-validation-valida√ß√£o-cruzada), [Quality Gates](#quality-gates)

---

## P

### Pandera
**Defini√ß√£o:** Biblioteca Python para valida√ß√£o de DataFrames (tipo Pydantic para dados).

**Por que usar:**
```python
# ‚ùå Sem valida√ß√£o
df = pd.read_csv('data.csv')
model.fit(df)  # üí• Quebra em produ√ß√£o se dados mudarem!

# ‚úÖ Com Pandera
schema.validate(df)  # Valida ANTES de treinar
model.fit(df)  # Seguro!
```

**Exemplo:**
```python
import pandera as pa
from pandera import Column, Check

# Definir schema
schema = pa.DataFrameSchema({
    "age": Column(int, Check.in_range(0, 120)),
    "income": Column(float, Check.greater_than(0)),
    "city": Column(str, Check.isin(['SP', 'RJ', 'MG'])),
}, strict=True)

# Validar
try:
    schema.validate(df)
    print("‚úÖ Dados v√°lidos!")
except pa.errors.SchemaError as e:
    print(f"‚ùå Erro: {e}")
```

**No projeto:**
```python
# src/data/schemas.py

raw_schema = pa.DataFrameSchema({
    "pH": Column(float, Check.in_range(2.5, 4.5)),
    "alcohol": Column(float, Check.in_range(8, 15)),
    # ...
})
```

**Ver tamb√©m:** [Schema Validation](#schema-validation)

---

### Precision (Precis√£o)
**Defini√ß√£o:** Das predi√ß√µes positivas, quantas estavam corretas?

**F√≥rmula:**
```
Precision = TP / (TP + FP)
           verdadeiros positivos
           -----------------------
           todos os positivos preditos
```

**Exemplo:**
```
Detector de spam:

Predisse "spam" 100 vezes:
- 85 eram realmente spam (TP)
- 15 eram ham (FP - falso alarme!)

Precision = 85 / 100 = 0.85 (85%)
```

**Interpreta√ß√£o:**
```
Precision alta (90%+):
"Quando digo que √© spam, PROVAVELMENTE √© spam"
Poucos falsos positivos ‚úÖ

Precision baixa (50%):
"Quando digo que √© spam, pode n√£o ser..."
Muitos falsos positivos ‚ùå
```

**Quando otimizar Precision:**
- Custo de FP √© alto
- Exemplo: Aprovar fraude (perda de dinheiro)
- Exemplo: Enviar email para spam (usu√°rio perde email importante)

**Trade-off com Recall:**
```
Modelo conservador:
‚îú‚îÄ S√≥ prediz positivo quando MUITO confiante
‚îú‚îÄ Precision ALTA ‚úÖ
‚îî‚îÄ Recall BAIXO ‚ùå (perde muitos positivos)

Modelo agressivo:
‚îú‚îÄ Prediz positivo com pouca confian√ßa
‚îú‚îÄ Precision BAIXA ‚ùå
‚îî‚îÄ Recall ALTO ‚úÖ (pega quase tudo)
```

**Ver tamb√©m:** [Recall](#recall-sensibilidade), [F1-Score](#f1-score), [Confusion Matrix](#confusion-matrix)

---

### P-value
**Defini√ß√£o:** Probabilidade de observar um resultado t√£o extremo quanto o observado, assumindo que a hip√≥tese nula √© verdadeira.

**Tradu√ß√£o simples:** "Qu√£o improv√°vel √© esse resultado se n√£o houver efeito real?"

**Analogia - moeda:**
```
Voc√™ joga moeda 100 vezes:
Resultado: 90 caras, 10 coroas

Pergunta: "Essa moeda √© viciada?"

P-value responde:
"Se a moeda fosse JUSTA, qual probabilidade de ver 90/10?"

p-value = 0.0001 (muito baixo!)
Interpreta√ß√£o: "√â MUITO improv√°vel ver 90/10 em moeda justa"
Conclus√£o: Moeda provavelmente √© viciada! üé≤
```

**No contexto de Drift:**
```python
stat, p_value = ks_2samp(train_age, prod_age)

p_value = 0.001 (baixo)
Interpreta√ß√£o:
"√â muito improv√°vel que essas distribui√ß√µes sejam iguais"
Conclus√£o: DRIFT detectado! üö®

p_value = 0.73 (alto)
Interpreta√ß√£o:
"√â bem poss√≠vel que essas distribui√ß√µes sejam iguais"
Conclus√£o: Sem drift ‚úÖ
```

**Regra pr√°tica:**
```
p-value < 0.05: Rejeita hip√≥tese nula (h√° diferen√ßa!)
p-value ‚â• 0.05: N√£o rejeita (pode ser igual)
```

**CUIDADO:** P-value N√ÉO √© "probabilidade de estar errado"!

**Ver tamb√©m:** [KS Test](#ks-test-kolmogorov-smirnov), [Data Drift](#data-drift)

---

### Pytest
**Defini√ß√£o:** Framework de testes para Python.

**Exemplo b√°sico:**
```python
# test_example.py

def test_addition():
    assert 1 + 1 == 2

def test_list_length():
    my_list = [1, 2, 3]
    assert len(my_list) == 3
```

**Rodar:**
```bash
pytest test_example.py -v
```

**Fixtures:**
```python
import pytest

@pytest.fixture
def sample_data():
    return pd.DataFrame({'age': [25, 30, 35]})

def test_mean_age(sample_data):
    assert sample_data['age'].mean() == 30
```

**Parametrize:**
```python
@pytest.mark.parametrize("a,b,expected", [
    (1, 1, 2),
    (2, 3, 5),
    (10, 5, 15),
])
def test_addition(a, b, expected):
    assert a + b == expected
```

**Ver tamb√©m:** [TDD](#tdd-test-driven-development)

---

## Q

### Quality Gates
**Defini√ß√£o:** Thresholds m√≠nimos que o modelo DEVE atingir para ser considerado "production-ready".

**Exemplo neste projeto:**
```yaml
# params.yaml

metrics:
  min_accuracy: 0.75    # 75%
  min_precision: 0.73
  min_recall: 0.73
  min_f1: 0.73
  max_train_test_gap: 0.10  # M√°x 10% overfitting
```

**Implementa√ß√£o:**
```python
def validate_quality_gates(metrics, thresholds):
    passed = True
    
    if metrics['accuracy'] < thresholds['min_accuracy']:
        print("‚ùå Accuracy below threshold")
        passed = False
    
    if metrics['overfitting_gap'] > thresholds['max_gap']:
        print("‚ùå Overfitting detected")
        passed = False
    
    return passed

# No CI/CD:
if not validate_quality_gates(metrics, thresholds):
    exit(1)  # Fail pipeline!
```

**Por que importante:**
- ‚úÖ Evita deploy de modelos ruins
- ‚úÖ Padroniza√ß√£o (todos os modelos passam pelos mesmos crit√©rios)
- ‚úÖ Auditabilidade (compliance)

**Como definir thresholds:**
1. Baseline (modelo simples)
2. Requisitos de neg√≥cio
3. Benchmarks da literatura
4. A/B test com threshold gradualmente maior

**Ver tamb√©m:** [Accuracy](#accuracy-acur√°cia), [Overfitting](#overfitting)

---

## R

### Recall (Sensibilidade)
**Defini√ß√£o:** Dos casos positivos reais, quantos o modelo detectou?

**F√≥rmula:**
```
Recall = TP / (TP + FN)
        verdadeiros positivos
        ----------------------
        todos os positivos reais
```

**Exemplo:**
```
Detector de c√¢ncer:

100 pacientes COM c√¢ncer (ground truth):
- 90 foram detectados (TP)
- 10 foram perdidos! (FN)

Recall = 90 / 100 = 0.90 (90%)
```

**Interpreta√ß√£o:**
```
Recall alto (95%+):
"Pego QUASE TODOS os casos positivos"
Poucos falsos negativos ‚úÖ

Recall baixo (60%):
"Perco MUITOS casos positivos"
Muitos falsos negativos ‚ùå
```

**Quando otimizar Recall:**
- Custo de FN √© alto
- Exemplo: Diagn√≥stico de doen√ßa grave (perder um caso = fatal)
- Exemplo: Detec√ß√£o de fraude (perder fraude = perda financeira)

**Analogia:**
```
Pescador:
Precision = % de peixes vs lixo na rede
Recall = % de peixes do lago que voc√™ pegou

Recall alto: Pegou quase todos os peixes! (mas tamb√©m muito lixo)
Precision alto: S√≥ pegou peixes! (mas deixou muitos no lago)
```

**Ver tamb√©m:** [Precision](#precision-precis√£o), [F1-Score](#f1-score)

---

### Requirements.txt
**Defini√ß√£o:** Arquivo que lista todas as depend√™ncias Python do projeto.

**Exemplo:**
```
# requirements.txt
pandas==2.1.4
numpy==1.24.3
scikit-learn==1.3.2
pytest==7.4.3
```

**Gerar:**
```bash
# Instalar tudo que precisa
pip install pandas scikit-learn pytest

# Salvar vers√µes exatas
pip freeze > requirements.txt
```

**Usar:**
```bash
# Em nova m√°quina
pip install -r requirements.txt
```

**Por que vers√µes fixas:**
```
‚ùå pandas (sem vers√£o)
Problema: Update quebra c√≥digo!

‚úÖ pandas==2.1.4
Garantia: Sempre mesma vers√£o = funciona!
```

**Ver tamb√©m:** [Reprodutibilidade](#reprodutibilidade)

---

### Reprodutibilidade
**Defini√ß√£o:** Capacidade de obter exatamente o mesmo resultado ao repetir um experimento.

**Em ML, requer versionar:**
```
‚úÖ C√≥digo (Git)
‚úÖ Dados (DVC)
‚úÖ Hiperpar√¢metros (params.yaml)
‚úÖ Depend√™ncias (requirements.txt)
‚úÖ Ambiente (Docker)
‚úÖ Seeds (random_state=42)
```

**Exemplo:**
```bash
# Experimento de 6 meses atr√°s
git checkout <commit-hash>
dvc checkout
dvc repro

# Resultado: EXATAMENTE o mesmo modelo!
```

**Por que importante:**
- ‚úÖ Compliance (regula√ß√£o exige)
- ‚úÖ Debugging (reproduzir bug)
- ‚úÖ Ci√™ncia (valida√ß√£o externa)
- ‚úÖ Produ√ß√£o (deploy confi√°vel)

**Ver tamb√©m:** [DVC](#dvc-data-version-control), [MLflow](#mlflow)

---

## S

### Schema Validation
**Defini√ß√£o:** Validar que dados atendem estrutura esperada (colunas, tipos, constraints).

**Por que necess√°rio:**
```
‚ùå Sem valida√ß√£o:
df = pd.read_csv('data.csv')
model.fit(df)
# üí• Quebra em produ√ß√£o se:
#    - Coluna renomeada
#    - Tipo mudou (str ‚Üí float)
#    - Valores fora do range
```

**‚úÖ Com valida√ß√£o:**
```python
schema.validate(df)  # Fail fast se algo errado!
model.fit(df)
```

**Exemplo:**
```python
import pandera as pa

schema = pa.DataFrameSchema({
    "age": Column(int, Check.in_range(0, 120)),
    "income": Column(float, Check.greater_than(0)),
}, strict=True)  # strict: n√£o permite colunas extras
```

**Ver tamb√©m:** [Pandera](#pandera)

---

### Shadow Mode
**Defini√ß√£o:** Estrat√©gia de deploy onde novo modelo roda em paralelo ao antigo, mas N√ÉO serve usu√°rios (s√≥ loga predi√ß√µes).

**Como funciona:**
```
Request ‚Üí
    ‚îú‚îÄ Champion (v1) ‚Üí Serve user ‚úÖ
    ‚îî‚îÄ Challenger (v2) ‚Üí Only logs üìù (n√£o serve!)
```

**Exemplo:**
```python
def predict(request):
    # Champion serve usu√°rio
    prediction_v1 = model_v1.predict(request)
    
    # Challenger s√≥ loga (background)
    prediction_v2 = model_v2.predict(request)
    log_to_monitoring({
        'v1': prediction_v1,
        'v2': prediction_v2,
        'request_id': request.id
    })
    
    return prediction_v1  # Usu√°rio recebe v1
```

**Ap√≥s dias/semanas:**
```python
# Analisar logs
compare_predictions(v1_logs, v2_logs)

# Se v2 melhor:
promote_to_production(model_v2)
```

**Vantagens:**
- ‚úÖ **Zero risco** (usu√°rios n√£o afetados)
- ‚úÖ Valida√ß√£o com tr√°fego REAL
- ‚úÖ M√©tricas side-by-side

**Desvantagens:**
- ‚ùå **2x custo computacional** (roda 2 modelos)
- ‚ùå N√£o valida lat√™ncia real (n√£o est√° no critical path)

**Quando usar:**
- Sistemas cr√≠ticos (sa√∫de, finan√ßas)
- Custo de erro > custo de infra 2x

**Ver tamb√©m:** [Canary](#canary-deployment), [A/B Testing](#ab-testing)

---

## T

### TDD (Test-Driven Development)
**Defini√ß√£o:** Metodologia onde voc√™ escreve TESTES antes do c√≥digo.

**Ciclo Red-Green-Refactor:**
```
1. RED: Escrever teste (que falha)
   test_accuracy():
       assert accuracy > 0.9

2. GREEN: Escrever c√≥digo m√≠nimo (que passa)
   def train():
       return model_with_90_accuracy

3. REFACTOR: Melhorar c√≥digo
   def train():
       # C√≥digo limpo e otimizado
```

**Benef√≠cios:**
- ‚úÖ Testes garantem funcionalidade
- ‚úÖ C√≥digo test√°vel por design
- ‚úÖ Documenta√ß√£o viva

**Ver tamb√©m:** [Pytest](#pytest)

---

### Training-Serving Skew
**Defini√ß√£o:** Diferen√ßa entre dados/features usados no treino vs produ√ß√£o.

**Problema:**
```
TREINO:
features = calculate_features_v1(data)  # Python
model.fit(features)
Accuracy: 95% ‚úÖ

PRODU√á√ÉO:
features = calculate_features_v2(data)  # Java (reimplementado)
model.predict(features)
Accuracy: 70% üò± (WTF?!)

Causa: Features DIFERENTES!
```

**Exemplo real:**
```python
# TREINO (Data Scientist):
df['avg_7d'] = df['price'].rolling(7).mean()

# PRODU√á√ÉO (Engineer reimplementou):
avg = sum(last_7_prices) / 7  # BUG: n√£o usa padding!
```

**Solu√ß√£o: Feature Store**
```
Single source of truth para features
Treino e produ√ß√£o usam MESMAS features
```

**Ver tamb√©m:** [Feature Store](#feature-store)

---

## V

### Versionamento
**Defini√ß√£o:** Rastrear mudan√ßas ao longo do tempo.

**No CD4ML, versionar:**
```
1. C√≥digo ‚Üí Git
   git commit -m "Add feature X"

2. Dados ‚Üí DVC
   dvc add data/train.csv

3. Modelos ‚Üí DVC + MLflow
   dvc add models/model.pkl
   mlflow.sklearn.log_model(model, "model")

4. Hiperpar√¢metros ‚Üí params.yaml (Git)
   git add params.yaml

5. Ambiente ‚Üí requirements.txt + Docker
   pip freeze > requirements.txt
```

**Por que importante:**
```
Sem versionamento:
"Qual modelo est√° em produ√ß√£o?" ü§∑
"Quais dados usei h√° 6 meses?" ü§∑
"Por que accuracy caiu?" ü§∑

Com versionamento:
git log  ‚Üí Vejo c√≥digo exato
dvc log  ‚Üí Vejo dados exatos
mlflow ui ‚Üí Vejo experimentos
```

**Ver tamb√©m:** [Git](#git), [DVC](#dvc-data-version-control), [MLflow](#mlflow)

---

## W

### Workflow
**Defini√ß√£o:** Sequ√™ncia de passos automatizados no CI/CD.

**Exemplo GitHub Actions:**
```yaml
name: ML Pipeline

on: [push]

jobs:
  test-data:
    steps:
      - Run data tests
  
  train:
    needs: test-data
    steps:
      - Train model
  
  deploy:
    needs: train
    steps:
      - Deploy to prod
```

**Ver tamb√©m:** [GitHub Actions](#github-actions), [CI/CD](#cicd)

---

## üéì RESUMO DOS TERMOS MAIS IMPORTANTES

Para entrevista ThoughtWorks, memorize especialmente:

1. **CD4ML** - O conceito principal
2. **DVC** - Versionamento de dados
3. **Hash** - Como DVC identifica arquivos
4. **Drift** (3 tipos) - Data, Model, Concept
5. **CI/CD** - Automa√ß√£o
6. **Canary Deployment** - Deploy gradual
7. **Quality Gates** - Thresholds de produ√ß√£o
8. **Feature Store** - Evitar training-serving skew
9. **Precision vs Recall** - M√©tricas fundamentais
10. **Overfitting** - Problema comum

---

## üìö REFER√äNCIAS

- [Martin Fowler - CD4ML](https://martinfowler.com/articles/cd4ml.html)
- [Google - Rules of ML](https://developers.google.com/machine-learning/guides/rules-of-ml)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [DVC Documentation](https://dvc.org/doc)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)

---

<div align="center">

**üìñ Gloss√°rio criado para CD4ML Production Project**

*√öltima atualiza√ß√£o: Dezembro 2025*

</div>