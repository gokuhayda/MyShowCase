"""
FastAPI - Wine Quality Prediction API
======================================
API REST para servir modelo de classifica√ß√£o de qualidade de vinho.

Endpoints:
- GET  /              - Health check e info da API
- GET  /health        - Health check detalhado
- POST /predict       - Predi√ß√£o single
- POST /predict/batch - Predi√ß√£o em lote
- GET  /model/info    - Informa√ß√µes do modelo

Swagger UI: http://localhost:8000/docs
ReDoc: http://localhost:8000/redoc
"""

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
from datetime import datetime
import logging
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from api.models import (
    WineSample,
    WineBatchRequest,
    PredictionResponse,
    BatchPredictionResponse,
    HealthResponse,
    ModelInfoResponse
)
from api.predictor import PredictorService

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# FastAPI App Configuration
# ============================================================================

app = FastAPI(
    title="Wine Quality Prediction API",
    description="""
    üç∑ API REST para classifica√ß√£o de qualidade de vinho.
    
    ## Features
    * **Single Prediction**: Predi√ß√£o para uma amostra
    * **Batch Prediction**: Predi√ß√£o para m√∫ltiplas amostras
    * **Model Info**: Informa√ß√µes sobre o modelo em produ√ß√£o
    * **Health Check**: Status da API e do modelo
    
    ## ML Pipeline
    - Model: RandomForest Classifier
    - Features: 11 physicochemical properties
    - Target: Binary classification (good wine vs bad wine)
    - Accuracy: ~86%
    
    ## Usage
```python
    import requests
    
    # Single prediction
    sample = {
        "fixed_acidity": 7.4,
        "volatile_acidity": 0.7,
        # ... outras features
    }
    response = requests.post("http://localhost:8000/predict", json=sample)
    print(response.json())
```
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_tags=[
        {"name": "Health", "description": "Health checks e status"},
        {"name": "Prediction", "description": "Endpoints de predi√ß√£o"},
        {"name": "Model", "description": "Informa√ß√µes do modelo"}
    ]
)

# ============================================================================
# CORS Middleware
# ============================================================================
# Permitir requisi√ß√µes de qualquer origem (ajustar em produ√ß√£o!)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Em produ√ß√£o: especificar dom√≠nios permitidos
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# Global State - Predictor Service
# ============================================================================
predictor_service = None

@app.on_event("startup")
async def startup_event():
    """Inicializa o servi√ßo de predi√ß√£o na startup da API."""
    global predictor_service
    logger.info("üöÄ Starting API...")
    
    try:
        predictor_service = PredictorService()
        logger.info("‚úÖ Predictor service initialized successfully")
        logger.info(f"   Model loaded: {predictor_service.model_path}")
        logger.info(f"   Startup time: {datetime.now().isoformat()}")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize predictor: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup na shutdown da API."""
    logger.info("üõë Shutting down API...")
    # Cleanup se necess√°rio (fechar conex√µes, etc)

# ============================================================================
# ENDPOINTS - Health & Info
# ============================================================================

@app.get(
    "/",
    response_model=dict,
    tags=["Health"],
    summary="Root endpoint"
)
async def root():
    """
    Root endpoint - informa√ß√µes b√°sicas da API.
    """
    return {
        "message": "üç∑ Wine Quality Prediction API",
        "version": "1.0.0",
        "status": "healthy",
        "docs": "/docs",
        "health": "/health"
    }

@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["Health"],
    summary="Health check detalhado"
)
async def health_check():
    """
    Health check detalhado.
    
    Retorna:
    - Status da API
    - Status do modelo
    - M√©tricas de performance
    - Uptime
    """
    try:
        # Verificar se modelo est√° carregado
        is_model_loaded = predictor_service is not None and predictor_service.predictor is not None
        
        if not is_model_loaded:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Model not loaded"
            )
        
        # Teste r√°pido de predi√ß√£o (smoke test)
        test_sample = WineSample(
            fixed_acidity=7.4,
            volatile_acidity=0.7,
            citric_acid=0.0,
            residual_sugar=1.9,
            chlorides=0.076,
            free_sulfur_dioxide=11.0,
            total_sulfur_dioxide=34.0,
            density=0.9978,
            pH=3.51,
            sulphates=0.56,
            alcohol=9.4
        )
        
        import time
        start = time.time()
        _ = predictor_service.predict(test_sample)
        latency_ms = (time.time() - start) * 1000
        
        return HealthResponse(
            status="healthy",
            model_loaded=True,
            model_version=predictor_service.get_model_version(),
            latency_ms=round(latency_ms, 2),
            timestamp=datetime.now().isoformat()
        )
    
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Health check failed: {str(e)}"
        )

# ============================================================================
# ENDPOINTS - Predictions
# ============================================================================

@app.post(
    "/predict",
    response_model=PredictionResponse,
    tags=["Prediction"],
    summary="Predi√ß√£o single",
    status_code=status.HTTP_200_OK
)
async def predict(sample: WineSample):
    """
    Fazer predi√ß√£o para uma amostra de vinho.
    
    **Features necess√°rias (11):**
    - fixed_acidity: Acidez fixa (g/L)
    - volatile_acidity: Acidez vol√°til (g/L)
    - citric_acid: √Åcido c√≠trico (g/L)
    - residual_sugar: A√ß√∫car residual (g/L)
    - chlorides: Cloretos (g/L)
    - free_sulfur_dioxide: SO2 livre (mg/L)
    - total_sulfur_dioxide: SO2 total (mg/L)
    - density: Densidade (g/cm¬≥)
    - pH: pH
    - sulphates: Sulfatos (g/L)
    - alcohol: √Ålcool (%vol)
    
    **Retorna:**
    - prediction: 0 (bad wine) ou 1 (good wine)
    - confidence: Probabilidade [0-1]
    - probabilities: {0: prob_bad, 1: prob_good}
    - interpretation: Explica√ß√£o textual
    - latency_ms: Tempo de infer√™ncia
    
    **Exemplo:**
```json
    {
        "fixed_acidity": 7.4,
        "volatile_acidity": 0.7,
        "citric_acid": 0.0,
        "residual_sugar": 1.9,
        "chlorides": 0.076,
        "free_sulfur_dioxide": 11.0,
        "total_sulfur_dioxide": 34.0,
        "density": 0.9978,
        "pH": 3.51,
        "sulphates": 0.56,
        "alcohol": 9.4
    }
```
    """
    try:
        result = predictor_service.predict(sample)
        return result
    
    except ValueError as e:
        logger.error(f"Validation error: {e}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=str(e)
        )
    
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Prediction failed: {str(e)}"
        )

@app.post(
    "/predict/batch",
    response_model=BatchPredictionResponse,
    tags=["Prediction"],
    summary="Predi√ß√£o em lote",
    status_code=status.HTTP_200_OK
)
async def predict_batch(request: WineBatchRequest):
    """
    Fazer predi√ß√µes para m√∫ltiplas amostras de vinho.
    
    **Limite:** 100 amostras por request
    
    **Retorna:**
    - predictions: Lista de predi√ß√µes
    - total: Total de amostras
    - latency_ms: Tempo total de infer√™ncia
    - avg_latency_ms: Tempo m√©dio por amostra
    
    **Exemplo:**
```json
    {
        "samples": [
            {
                "fixed_acidity": 7.4,
                "volatile_acidity": 0.7,
                ...
            },
            {
                "fixed_acidity": 8.1,
                "volatile_acidity": 0.6,
                ...
            }
        ]
    }
```
    """
    try:
        # Validar tamanho do batch
        if len(request.samples) > 100:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Batch size exceeds maximum of 100 samples"
            )
        
        if len(request.samples) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Batch must contain at least 1 sample"
            )
        
        result = predictor_service.predict_batch(request.samples)
        return result
    
    except HTTPException:
        raise
    
    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Batch prediction failed: {str(e)}"
        )

# ============================================================================
# ENDPOINTS - Model Info
# ============================================================================

@app.get(
    "/model/info",
    response_model=ModelInfoResponse,
    tags=["Model"],
    summary="Informa√ß√µes do modelo"
)
async def model_info():
    """
    Obter informa√ß√µes sobre o modelo em produ√ß√£o.
    
    Retorna:
    - model_version: Vers√£o do modelo
    - model_type: Tipo de algoritmo
    - features: Lista de features esperadas
    - metrics: M√©tricas de performance
    - trained_at: Data/hora do treino
    """
    try:
        info = predictor_service.get_model_info()
        return info
    
    except Exception as e:
        logger.error(f"Error getting model info: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get model info: {str(e)}"
        )

# ============================================================================
# Error Handlers
# ============================================================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Handler customizado para HTTPException."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.now().isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """Handler gen√©rico para exce√ß√µes n√£o tratadas."""
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": "Internal server error",
            "detail": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

# ============================================================================
# Run Server (para desenvolvimento)
# ============================================================================

if __name__ == "__main__":
    uvicorn.run(
        "api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # Auto-reload em desenvolvimento
        log_level="info"
    )