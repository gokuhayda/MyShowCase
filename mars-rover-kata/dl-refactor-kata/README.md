# üî• Deep Learning Trainer Refactoring Kata

[![Tests](https://github.com/seu-usuario/dl-refactor-kata/actions/workflows/tests.yml/badge.svg)](https://github.com/seu-usuario/dl-refactor-kata/actions)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Demonstra√ß√£o de **Software Engineering aplicado a Deep Learning**: como transformar c√≥digo espaguete de treinamento em sistema extens√≠vel e test√°vel usando **Callback Pattern**.

---

## üéØ Objetivo

Este projeto demonstra a aplica√ß√£o de princ√≠pios SOLID e Design Patterns em c√≥digo de Machine Learning, especificamente:

- ‚úÖ **Strategy Pattern** via Callbacks
- ‚úÖ **Open/Closed Principle** (extens√≠vel sem modificar core)
- ‚úÖ **Dependency Inversion** (abstra√ß√µes antes de implementa√ß√µes)
- ‚úÖ **Testabilidade** (callbacks isolados do loop de treino)

---

## üìñ O Problema

### ‚ùå C√≥digo Espaguete (Antes)
```python
# T√≠pico c√≥digo de Kaggle/Research
for epoch in range(100):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        optimizer.step()

    # Mistura I/O, logging e l√≥gica de parada
    if epoch % 10 == 0:
        print(f"Epoch {epoch} loss={loss}")
        torch.save(model, "ckpt.pth")
    if loss < 0.01:
        break  # Early stopping hardcoded
```

**Problemas:**
- üö´ N√£o test√°vel (precisa rodar rede inteira)
- üö´ N√£o extens√≠vel (adicionar Slack notification = mexer no loop)
- üö´ Responsabilidades misturadas (matem√°tica + I/O + controle)

---

### ‚úÖ Solu√ß√£o com Trainer Pattern (Depois)
```python
trainer = Trainer(
    model=MyModel(),
    optimizer=Adam(),
    loss_fn=MSELoss(),
    callbacks=[
        EarlyStopping(patience=5),
        ModelCheckpoint(filepath="best_model.pth"),
        TensorBoardLogger(),
        # Quer Slack? S√≥ adicionar: SlackNotifier()
    ]
)
trainer.fit(train_loader, epochs=100)
```

**Benef√≠cios:**
- ‚úÖ Loop matem√°tico limpo e puro
- ‚úÖ Extens√≠vel via composi√ß√£o (n√£o heran√ßa)
- ‚úÖ Cada callback test√°vel isoladamente
- ‚úÖ Segue Open/Closed Principle (SOLID)

---

## üöÄ Instala√ß√£o

### Op√ß√£o 1: Poetry (recomendado)
```bash
git clone https://github.com/seu-usuario/dl-refactor-kata.git
cd dl-refactor-kata
poetry install
poetry shell
```

### Op√ß√£o 2: pip
```bash
git clone https://github.com/seu-usuario/dl-refactor-kata.git
cd dl-refactor-kata
pip install -e .
```

---

## üíª Uso

### Exemplo B√°sico
```python
from dl_trainer import Trainer
from dl_trainer.callbacks import EarlyStopping, ModelCheckpoint

# Configura√ß√£o via composi√ß√£o
trainer = Trainer(
    model="SimulatedModel",
    optimizer="Adam",
    loss_fn="MSE",
    callbacks=[
        EarlyStopping(patience=3),
        ModelCheckpoint(filepath="model.pth")
    ]
)

# Dados simulados (para demo sem PyTorch)
fake_dataloader = [1, 2, 3]

# Rodar treino
trainer.fit(fake_dataloader, epochs=10)
```

**Sa√≠da esperada:**

```
üöÄ Training started with 2 callbacks
Epoch 0 | Loss: 0.30
üíæ Checkpoint: Saving model to model.pth
Epoch 1 | Loss: 0.30
üõë Early stopping triggered at epoch 1 (patience: 3)
```

### Exemplo Avan√ßado (M√∫ltiplos Callbacks)

Ver `examples/advanced_usage.py` para:
- Custom metrics logging
- Learning rate scheduling
- Gradient clipping
- Slack notifications

---

## üß™ Testes

Executar todos os testes:
```bash
pytest
```

Com coverage:
```bash
pytest --cov=src/dl_trainer --cov-report=html
```

Watch mode (rodar a cada mudan√ßa):
```bash
pytest-watch
```

---

## üìö Arquitetura

### Diagrama de Classes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Trainer      ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ + fit()         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ + _notify()     ‚îÇ       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ usa                    ‚îÇ
‚ñº                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ <<Protocol>>    ‚îÇ      ‚îÇ
‚îÇ   Callback      ‚îÇ      ‚îÇ
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ      ‚îÇ
‚îÇ + on_train_begin‚îÇ      ‚îÇ
‚îÇ + on_epoch_end  ‚îÇ      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚ñ≤                        ‚îÇ
‚îÇ implementa             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                ‚îÇ                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇEarlyStopping  ‚îÇ  ‚îÇModelCheckpoint‚îÇ ‚îÇCustomCallback‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Design Patterns Aplicados

1. **Strategy Pattern**: Cada callback √© uma estrat√©gia intercambi√°vel
2. **Observer Pattern**: Trainer notifica eventos para callbacks
3. **Template Method**: `fit()` define estrutura, callbacks customizam comportamento
4. **Dependency Inversion**: Trainer depende da abstra√ß√£o `Callback`, n√£o de implementa√ß√µes concretas

Ver `docs/PATTERNS.md` para detalhes.

---

## üéì Conceitos Demonstrados

### 1. Open/Closed Principle (SOLID)
```python
# Aberto para extens√£o (adicionar SlackCallback)
# Fechado para modifica√ß√£o (Trainer n√£o muda)
class SlackNotifier(Callback):
    def on_train_begin(self, logs):
        send_slack("Treino iniciado!")
    def on_epoch_end(self, epoch, logs):
        send_slack(f"√âpoca {epoch} conclu√≠da")
        return False

# Uso sem mudar Trainer
trainer = Trainer(
    callbacks=[SlackNotifier()]  # S√≥ adicionar!
)
```

### 2. Single Responsibility Principle

Cada classe tem **uma** responsabilidade:
- `Trainer`: Executar loop de treino
- `EarlyStopping`: Decidir quando parar
- `ModelCheckpoint`: Salvar modelo

### 3. Testabilidade
```python
# Testar early stopping SEM treinar rede neural!
def test_early_stopping():
    callback = EarlyStopping(patience=2)
    # Simular √©pocas ruins
    callback.on_epoch_end(0, {"loss": 1.0})
    should_stop = callback.on_epoch_end(1, {"loss": 1.0})
    assert should_stop is True
```

---

## üîß Extens√µes Poss√≠veis

Exemplos de callbacks que voc√™ pode adicionar:

- **LearningRateScheduler**: Ajustar LR dinamicamente
- **GradientClipper**: Limitar gradientes
- **MetricsLogger**: Log em W&B, MLflow, TensorBoard
- **ProgressBar**: UI com tqdm
- **EmailNotifier**: Avisar quando treino terminar
- **ProfilerCallback**: Detectar bottlenecks

Todos seguem a mesma interface `Callback`.

---

## üìñ Refer√™ncias

Este padr√£o √© inspirado em:
- **Keras**: `model.fit(callbacks=[...])`
- **PyTorch Lightning**: `Trainer(callbacks=[...])`
- **FastAI**: `Learner.fit(..., cbs=[...])`

---

## ü§ù Contribuindo

1. Fork o projeto
2. Crie branch: `git checkout -b feature/novo-callback`
3. Commit: `git commit -m 'Add: GradientClipper callback'`
4. Push: `git push origin feature/novo-callback`
5. Abra Pull Request

---

## üìù Licen√ßa

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

## üë§ Autor

**[Seu Nome]**
- GitHub: [@seu-usuario](https://github.com/seu-usuario)
- LinkedIn: [seu-perfil](https://linkedin.com/in/seu-perfil)

---

## üéØ ThoughtWorks Context

Este projeto foi criado como parte da prepara√ß√£o para entrevistas t√©cnicas onde:
- ‚úÖ Clean Code e SOLID s√£o esperados
- ‚úÖ Cientistas de dados devem pensar como engenheiros
- ‚úÖ C√≥digo de ML deve ser test√°vel e manuten√≠vel

> "Move code out of notebooks into Python modules as early as possible. That way, they can rest within the safe confines of unit tests and domain boundaries." - ThoughtWorks Blog

---

**‚≠ê Se este projeto te ajudou, considere dar uma estrela!**

