"""
ImplementaÃ§Ãµes concretas de Callbacks.

Cada callback encapsula UMA responsabilidade especÃ­fica.
"""

from typing import Dict, Any


class EarlyStopping:
    """
    Para o treino se a mÃ©trica nÃ£o melhorar por N Ã©pocas.
    
    Strategy Pattern: Encapsula a lÃ³gica de "quando parar".
    """
    
    def __init__(self, patience: int = 3, min_delta: float = 0.0):
        """
        Args:
            patience: NÃºmero de Ã©pocas para esperar melhoria
            min_delta: Melhoria mÃ­nima considerada significativa
        """
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.bad_epochs = 0
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """Reseta contadores no inÃ­cio do treino."""
        self.best_loss = float('inf')
        self.bad_epochs = 0
        print(f"ðŸ“Š Early stopping: patience={self.patience}")
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> bool:
        """
        Verifica se houve melhoria na loss.
        
        Returns:
            True se deve parar o treino
        """
        current_loss = logs.get("loss", float('inf'))
        
        # Verifica se houve melhoria significativa
        if current_loss < (self.best_loss - self.min_delta):
            self.best_loss = current_loss
            self.bad_epochs = 0
            print(f"âœ¨ New best loss: {self.best_loss:.4f}")
        else:
            self.bad_epochs += 1
            print(
                f"ðŸ›‘ Early stopping: "
                f"{self.bad_epochs}/{self.patience} bad epochs"
            )
        
        # Para se atingiu o limite de paciÃªncia
        if self.bad_epochs >= self.patience:
            print(
                f"â¹ï¸  Early stopping triggered at epoch {epoch} "
                f"(patience: {self.patience})"
            )
            return True
        
        return False


class ModelCheckpoint:
    """
    Salva o modelo periodicamente ou quando melhora.
    
    Separa a lÃ³gica de I/O da lÃ³gica matemÃ¡tica.
    """
    
    def __init__(
        self, 
        filepath: str = "model_checkpoint.pth",
        save_best_only: bool = True
    ):
        """
        Args:
            filepath: Caminho para salvar o modelo
            save_best_only: Se True, sÃ³ salva quando melhora
        """
        self.filepath = filepath
        self.save_best_only = save_best_only
        self.best_loss = float('inf')
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """Inicializa o melhor loss."""
        self.best_loss = float('inf')
        print(f"ðŸ’¾ Checkpoint: saving to {self.filepath}")
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> bool:
        """
        Salva o modelo se for o melhor atÃ© agora.
        
        Na implementaÃ§Ã£o real:
```python
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': current_loss,
        }, self.filepath)
```
        """
        current_loss = logs.get("loss", float('inf'))
        
        should_save = (
            not self.save_best_only or 
            current_loss < self.best_loss
        )
        
        if should_save:
            if current_loss < self.best_loss:
                self.best_loss = current_loss
                print(
                    f"ðŸ’¾ Checkpoint: Saving best model "
                    f"(loss: {current_loss:.4f})"
                )
            else:
                print(f"ðŸ’¾ Checkpoint: Saving model at epoch {epoch}")
            
            # SimulaÃ§Ã£o de salvamento
            # torch.save(model.state_dict(), self.filepath)
        
        return False


class MetricsLogger:
    """
    Loga mÃ©tricas em formato estruturado.
    
    Pode ser extendido para enviar para W&B, MLflow, TensorBoard.
    """
    
    def __init__(self, log_every_n_epochs: int = 1):
        """
        Args:
            log_every_n_epochs: FrequÃªncia de logging
        """
        self.log_every_n_epochs = log_every_n_epochs
        self.history = []
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """Inicializa histÃ³rico."""
        self.history = []
        print("ðŸ“ˆ Metrics logger initialized")
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> bool:
        """Registra mÃ©tricas."""
        self.history.append({"epoch": epoch, **logs})
        
        if epoch % self.log_every_n_epochs == 0:
            print(f"ðŸ“Š Metrics: {logs}")
        
        return False
    
    def get_history(self) -> list:
        """Retorna histÃ³rico de mÃ©tricas."""
        return self.history


class LearningRateScheduler:
    """
    Ajusta learning rate ao longo do treino.
    
    Strategy Pattern: Encapsula polÃ­tica de scheduling.
    """
    
    def __init__(self, initial_lr: float = 0.001, decay_factor: float = 0.9):
        """
        Args:
            initial_lr: Learning rate inicial
            decay_factor: Fator de decaimento por Ã©poca
        """
        self.initial_lr = initial_lr
        self.decay_factor = decay_factor
        self.current_lr = initial_lr
    
    def on_train_begin(self, logs: Dict[str, Any]) -> None:
        """Reseta learning rate."""
        self.current_lr = self.initial_lr
        print(f"ðŸ“‰ LR Scheduler: initial_lr={self.initial_lr}")
    
    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> bool:
        """Decai o learning rate."""
        self.current_lr *= self.decay_factor
        print(f"ðŸ“‰ LR: {self.current_lr:.6f}")
        
        # Na implementaÃ§Ã£o real:
        # for param_group in optimizer.param_groups:
        #     param_group['lr'] = self.current_lr
        
        return False
