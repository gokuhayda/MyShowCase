{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H-AKORN: Hyperbolic Attention with Kuramoto Oscillator Regularized Networks\n",
    "\n",
    "**Author:** Éric Gustavo Reis de Sena  \n",
    "**Date:** January 2026\n",
    "\n",
    "This notebook demonstrates the H-AKORN transformer, which combines:\n",
    "1. **Hyperbolic geometry** for hierarchical representation\n",
    "2. **Kuramoto oscillator dynamics** for phase synchronization\n",
    "3. **Adaptive coupling** between attention heads\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input → Embeddings → H-AKORN Layers → LM Head → Output\n",
    "                           ↓\n",
    "                    [Hyperbolic Attention]\n",
    "                           ↓\n",
    "                  [Kuramoto Phase Dynamics]\n",
    "                           ↓\n",
    "                   [Adaptive Coupling]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "# !pip install torch numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import H-AKORN modules\n",
    "from hakorn import (\n",
    "    HAKORNTransformer,\n",
    "    HAKORNLoss,\n",
    "    KuramotoPhaseEvolution,\n",
    "    AdaptiveCoupling,\n",
    "    HyperbolicKuramotoAttention,\n",
    ")\n",
    "\n",
    "print(\"H-AKORN modules loaded successfully!\")\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Kuramoto Dynamics\n",
    "\n",
    "The Kuramoto model describes how coupled oscillators synchronize:\n",
    "\n",
    "$$\\frac{d\\theta_i}{dt} = \\omega_i + \\frac{K}{N} \\sum_j A_{ij} \\sin(\\theta_j - \\theta_i)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_i$: phase of oscillator $i$\n",
    "- $\\omega_i$: natural frequency\n",
    "- $K$: coupling strength\n",
    "- $A_{ij}$: coupling matrix\n",
    "\n",
    "The **order parameter** $r$ measures synchronization:\n",
    "\n",
    "$$r = \\left| \\frac{1}{N} \\sum_j e^{i\\theta_j} \\right|$$\n",
    "\n",
    "where $r \\in [0, 1]$ ($r=1$ = perfect sync, $r=0$ = no sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Kuramoto dynamics\n",
    "def visualize_kuramoto_evolution(num_heads=8, num_steps=100, coupling_strength=1.0):\n",
    "    \"\"\"\n",
    "    Visualize phase evolution of Kuramoto oscillators.\n",
    "    \"\"\"\n",
    "    # Create phase evolution module\n",
    "    phase_evolution = KuramotoPhaseEvolution(\n",
    "        d_model=768,\n",
    "        num_heads=num_heads,\n",
    "        coupling_strength=coupling_strength,\n",
    "        dt=0.05,\n",
    "    )\n",
    "    \n",
    "    # Create uniform coupling matrix\n",
    "    coupling_matrix = torch.ones(num_heads, num_heads) / num_heads\n",
    "    \n",
    "    # Evolve phases\n",
    "    phases_history = []\n",
    "    order_params = []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        phases, order_param = phase_evolution(coupling_matrix, batch_size=1)\n",
    "        phases_history.append(phases[0].cpu().numpy())\n",
    "        order_params.append(order_param[0].item())\n",
    "    \n",
    "    phases_history = np.array(phases_history)\n",
    "    order_params = np.array(order_params)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Phase evolution\n",
    "    for i in range(num_heads):\n",
    "        axes[0].plot(phases_history[:, i], label=f'Head {i+1}')\n",
    "    axes[0].set_xlabel('Time Step')\n",
    "    axes[0].set_ylabel('Phase (radians)')\n",
    "    axes[0].set_title('Phase Evolution')\n",
    "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Order parameter\n",
    "    axes[1].plot(order_params, linewidth=2, color='red')\n",
    "    axes[1].set_xlabel('Time Step')\n",
    "    axes[1].set_ylabel('Order Parameter r')\n",
    "    axes[1].set_title(f'Synchronization (K={coupling_strength})')\n",
    "    axes[1].set_ylim([0, 1.1])\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1.0, color='green', linestyle='--', label='Perfect Sync')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final order parameter: {order_params[-1]:.4f}\")\n",
    "    print(f\"Average order parameter: {order_params.mean():.4f}\")\n",
    "\n",
    "# Visualize\n",
    "visualize_kuramoto_evolution(num_heads=8, num_steps=200, coupling_strength=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create H-AKORN Model\n",
    "\n",
    "Initialize a small H-AKORN transformer for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': 50257,  # GPT-2 vocab size\n",
    "    'd_model': 512,\n",
    "    'num_layers': 6,\n",
    "    'num_heads': 8,\n",
    "    'd_ff': 2048,\n",
    "    'max_position_embeddings': 256,\n",
    "    'dropout': 0.1,\n",
    "    'curvature': -1.0,\n",
    "    'coupling_strength': 1.0,\n",
    "    'use_phase_modulation': True,\n",
    "}\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HAKORNTransformer(**config).to(device)\n",
    "\n",
    "print(f\"Model created with {model.get_num_params():,} parameters\")\n",
    "print(f\"Non-embedding parameters: {model.get_num_params(non_embedding=True):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Forward Pass\n",
    "\n",
    "Perform a forward pass and inspect outputs, including phase dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "batch_size = 4\n",
    "seq_length = 32\n",
    "input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_length)).to(device)\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Forward Pass Output ===\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\")\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "print(f\"Hidden states shape: {output['hidden_states'].shape}\")\n",
    "print(f\"Number of layers: {len(output['all_phases'])}\")\n",
    "\n",
    "# Analyze phase dynamics\n",
    "print(\"\\n=== Phase Dynamics ===\")\n",
    "for layer_idx, (phases, order_param) in enumerate(zip(output['all_phases'], output['all_order_params'])):\n",
    "    print(f\"Layer {layer_idx + 1}:\")\n",
    "    print(f\"  Phases shape: {phases.shape}\")\n",
    "    print(f\"  Order parameter: {order_param.mean().item():.4f}\")\n",
    "    print(f\"  Phase range: [{phases.min().item():.2f}, {phases.max().item():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Attention and Phase Coupling\n",
    "\n",
    "Visualize how attention patterns relate to phase synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_and_phases(output, layer_idx=0, batch_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns and phase values for a specific layer.\n",
    "    \"\"\"\n",
    "    if output['all_attentions'] is None:\n",
    "        print(\"No attention weights available. Set output_attentions=True\")\n",
    "        return\n",
    "    \n",
    "    attention = output['all_attentions'][layer_idx][batch_idx].cpu().numpy()\n",
    "    phases = output['all_phases'][layer_idx][batch_idx].cpu().numpy()\n",
    "    \n",
    "    num_heads = attention.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Average attention pattern\n",
    "    avg_attention = attention.mean(axis=0)\n",
    "    im1 = axes[0].imshow(avg_attention, cmap='viridis', aspect='auto')\n",
    "    axes[0].set_title(f'Average Attention Pattern (Layer {layer_idx + 1})')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Phase values per head\n",
    "    axes[1].bar(range(num_heads), phases)\n",
    "    axes[1].set_title(f'Phase Values (Layer {layer_idx + 1})')\n",
    "    axes[1].set_xlabel('Head Index')\n",
    "    axes[1].set_ylabel('Phase (radians)')\n",
    "    axes[1].set_ylim([0, 2*np.pi])\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Phase coherence (pairwise phase differences)\n",
    "    phase_diff = np.abs(phases[:, None] - phases[None, :])\n",
    "    phase_coherence = np.cos(phase_diff)\n",
    "    im3 = axes[2].imshow(phase_coherence, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[2].set_title(f'Phase Coherence Matrix (Layer {layer_idx + 1})')\n",
    "    axes[2].set_xlabel('Head Index')\n",
    "    axes[2].set_ylabel('Head Index')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize layer 0 and layer 3\n",
    "print(\"=== Layer 0 ===\")\n",
    "visualize_attention_and_phases(output, layer_idx=0)\n",
    "\n",
    "print(\"\\n=== Layer 3 ===\")\n",
    "visualize_attention_and_phases(output, layer_idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with H-AKORN Loss\n",
    "\n",
    "Demonstrate training with H-AKORN-specific regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss criterion\n",
    "criterion = HAKORNLoss(\n",
    "    lambda_sync=0.1,\n",
    "    lambda_variance=0.05,\n",
    ")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Simple training loop\n",
    "num_steps = 100\n",
    "log_interval = 10\n",
    "\n",
    "model.train()\n",
    "loss_history = []\n",
    "sync_history = []\n",
    "order_param_history = []\n",
    "\n",
    "print(\"\\n=== Training Loop ===\")\n",
    "for step in tqdm(range(num_steps)):\n",
    "    # Generate random input\n",
    "    input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_length)).to(device)\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    lm_loss = output['loss']\n",
    "    order_parameters = output['all_order_params']\n",
    "    \n",
    "    # Compute H-AKORN loss\n",
    "    loss_dict = criterion(lm_loss, order_parameters)\n",
    "    loss = loss_dict['total']\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log\n",
    "    loss_history.append(loss.item())\n",
    "    sync_history.append(loss_dict['sync'])\n",
    "    avg_order_param = torch.stack(order_parameters).mean().item()\n",
    "    order_param_history.append(avg_order_param)\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}: Loss={loss.item():.4f}, LM={loss_dict['lm']:.4f}, \"\n",
    "              f\"Sync={loss_dict['sync']:.4f}, Order={avg_order_param:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(loss_history)\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sync loss\n",
    "axes[1].plot(sync_history, color='orange')\n",
    "axes[1].set_title('Synchronization Loss')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('L_sync')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Order parameter\n",
    "axes[2].plot(order_param_history, color='green')\n",
    "axes[2].set_title('Order Parameter Evolution')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('r')\n",
    "axes[2].set_ylim([0, 1.1])\n",
    "axes[2].axhline(y=1.0, color='red', linestyle='--', label='Perfect Sync')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal order parameter: {order_param_history[-1]:.4f}\")\n",
    "print(f\"Order parameter improvement: {order_param_history[-1] - order_param_history[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation with H-AKORN\n",
    "\n",
    "Test generation capabilities (requires proper tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation example\n",
    "model.eval()\n",
    "\n",
    "# Create prompt (using random tokens as placeholder)\n",
    "prompt = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "print(f\"Generated sequence shape: {generated.shape}\")\n",
    "print(f\"Generated tokens: {generated[0].tolist()[:20]}...\")  # First 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase Dynamics Analysis\n",
    "\n",
    "Analyze how phases evolve across layers during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_phase_progression():\n",
    "    \"\"\"\n",
    "    Analyze how phases and order parameters evolve across layers.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.reset_phases()\n",
    "    \n",
    "    # Forward pass\n",
    "    input_ids = torch.randint(0, config['vocab_size'], (1, 64)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    \n",
    "    # Extract phase information\n",
    "    all_phases = output['all_phases']\n",
    "    all_order_params = output['all_order_params']\n",
    "    \n",
    "    num_layers = len(all_phases)\n",
    "    num_heads = all_phases[0].shape[1]\n",
    "    \n",
    "    # Convert to numpy\n",
    "    phases_array = np.array([p[0].cpu().numpy() for p in all_phases])  # [L, H]\n",
    "    order_params_array = np.array([o[0].cpu().item() for o in all_order_params])  # [L]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Phase evolution across layers\n",
    "    im = axes[0].imshow(phases_array.T, aspect='auto', cmap='twilight', vmin=0, vmax=2*np.pi)\n",
    "    axes[0].set_title('Phase Values Across Layers')\n",
    "    axes[0].set_xlabel('Layer Index')\n",
    "    axes[0].set_ylabel('Head Index')\n",
    "    plt.colorbar(im, ax=axes[0], label='Phase (radians)')\n",
    "    \n",
    "    # Order parameter across layers\n",
    "    axes[1].plot(range(num_layers), order_params_array, marker='o', linewidth=2)\n",
    "    axes[1].set_title('Order Parameter Across Layers')\n",
    "    axes[1].set_xlabel('Layer Index')\n",
    "    axes[1].set_ylabel('Order Parameter r')\n",
    "    axes[1].set_ylim([0, 1.1])\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1.0, color='red', linestyle='--', label='Perfect Sync')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n=== Phase Statistics ===\")\n",
    "    print(f\"Layer 0 order parameter: {order_params_array[0]:.4f}\")\n",
    "    print(f\"Layer {num_layers-1} order parameter: {order_params_array[-1]:.4f}\")\n",
    "    print(f\"Average order parameter: {order_params_array.mean():.4f}\")\n",
    "    print(f\"Order parameter improvement: {order_params_array[-1] - order_params_array[0]:.4f}\")\n",
    "\n",
    "analyze_layer_phase_progression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare with/without Phase Modulation\n",
    "\n",
    "Compare attention behavior with and without phase modulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phase_modulation():\n",
    "    \"\"\"\n",
    "    Compare models with and without phase modulation.\n",
    "    \"\"\"\n",
    "    # Create two models\n",
    "    config_with_phase = config.copy()\n",
    "    config_with_phase['use_phase_modulation'] = True\n",
    "    config_with_phase['num_layers'] = 4  # Smaller for comparison\n",
    "    \n",
    "    config_without_phase = config.copy()\n",
    "    config_without_phase['use_phase_modulation'] = False\n",
    "    config_without_phase['num_layers'] = 4\n",
    "    \n",
    "    model_with = HAKORNTransformer(**config_with_phase).to(device).eval()\n",
    "    model_without = HAKORNTransformer(**config_without_phase).to(device).eval()\n",
    "    \n",
    "    # Forward pass\n",
    "    input_ids = torch.randint(0, config['vocab_size'], (1, 32)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_with = model_with(input_ids, return_dict=True)\n",
    "        output_without = model_without(input_ids, return_dict=True)\n",
    "    \n",
    "    # Compare order parameters\n",
    "    order_with = [o[0].item() for o in output_with['all_order_params']]\n",
    "    order_without = [o[0].item() for o in output_without['all_order_params']]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = range(len(order_with))\n",
    "    ax.plot(x, order_with, marker='o', label='With Phase Modulation', linewidth=2)\n",
    "    ax.plot(x, order_without, marker='s', label='Without Phase Modulation', linewidth=2)\n",
    "    \n",
    "    ax.set_title('Order Parameter Comparison')\n",
    "    ax.set_xlabel('Layer Index')\n",
    "    ax.set_ylabel('Order Parameter r')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== Comparison Statistics ===\")\n",
    "    print(f\"With phase modulation:\")\n",
    "    print(f\"  Average order parameter: {np.mean(order_with):.4f}\")\n",
    "    print(f\"  Final order parameter: {order_with[-1]:.4f}\")\n",
    "    print(f\"\\nWithout phase modulation:\")\n",
    "    print(f\"  Average order parameter: {np.mean(order_without):.4f}\")\n",
    "    print(f\"  Final order parameter: {order_without[-1]:.4f}\")\n",
    "\n",
    "compare_phase_modulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"hakorn_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load(save_path)\n",
    "loaded_model = HAKORNTransformer(**checkpoint['config']).to(device)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Model loaded from {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ **Kuramoto phase dynamics** for attention head synchronization\n",
    "2. ✅ **Hyperbolic attention** with geodesic distances\n",
    "3. ✅ **Adaptive coupling** between oscillators\n",
    "4. ✅ **H-AKORN loss** with synchronization regularization\n",
    "5. ✅ **Phase-modulated attention** for improved representation\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Integration with CGT project**: Use `LorentzSubstrateHardened` for hyperbolic operations\n",
    "- **Distillation from GPT-2**: Use `TeacherDistillationLoss` from `hyperbolic_lm_losses.py`\n",
    "- **Large-scale training**: Use `train_hakorn.py` script\n",
    "- **Real tokenization**: Integrate with GPT-2 tokenizer\n",
    "- **Evaluation**: Test on downstream NLP tasks\n",
    "\n",
    "### Key References:\n",
    "\n",
    "- Kuramoto, Y. (1975). Self-entrainment of a population of coupled non-linear oscillators\n",
    "- Nickel & Kiela (2017). Poincaré Embeddings for Learning Hierarchical Representations\n",
    "- Ganea et al. (2018). Hyperbolic Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Éric Gustavo Reis de Sena  \n",
    "**License:** CC-BY-NC-SA-4.0  \n",
    "**Contact:** eirikreisena@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
