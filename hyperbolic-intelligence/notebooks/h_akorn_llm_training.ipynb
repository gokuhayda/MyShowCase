{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q82RVxengkh"
      },
      "source": [
        "## üéì Distillation: GPT-2 ‚Üí H-AKORN H-LLM\n",
        "\n",
        "Transferir conhecimento do GPT-2 pr√©-treinado para o modelo hiperb√≥lico H-AKORN.\n",
        "\n",
        "**Teacher**: GPT-2 (124M params, Euclidean)\n",
        "**Student**: H-AKORN H-LLM (~30M params, Hyperbolic + Kuramoto)\n",
        "\n",
        "**Distillation loss**:\n",
        "```\n",
        "L_distill = Œ±¬∑KL(student || teacher) + (1-Œ±)¬∑L_LM + Œª_hidden¬∑L_hidden\n",
        "```\n",
        "\n",
        "Onde:\n",
        "- KL divergence alinha distribui√ß√µes de output\n",
        "- L_LM √© a standard language modeling loss\n",
        "- L_hidden alinha representa√ß√µes escondidas (Euclidean ‚Üí Hyperbolic)\n",
        "\n",
        "## üîÅ Fluxo de Treinamento do H-LLM\n",
        "\n",
        "```text\n",
        "Modelo inicial (H-LLM random / pr√©-treino)\n",
        "        ‚Üì\n",
        "[ FASE 1 ] Destila√ß√£o (Teacher ON)\n",
        "        ‚Üì\n",
        "[ FASE 2 ] LM-only (Teacher OFF)\n",
        "        ‚Üì\n",
        "[ FASE 3 ] Avalia√ß√£o / uso / refinamento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H7oGEbVECr_X",
        "outputId": "c2c31040-5585-47ab-a275-552689244fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Checkpoints deletados\n"
          ]
        }
      ],
      "source": [
        "# @title Deletar checkpoints antigos\n",
        "#!rm -rf /content/outputs/h_llm/checkpoints/*\n",
        "print(\"‚úÖ Checkpoints deletados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "collapsed": true,
        "id": "hBhB2ogNngki",
        "outputId": "86631f57-beed-4aab-ac17-a1cd99b13908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Upload cgt_project_with_hakorn.zip:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6859927a-3abd-4569-a613-7a897d5f2aea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6859927a-3abd-4569-a613-7a897d5f2aea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cgt_project_with_hakorn.zip to cgt_project_with_hakorn.zip\n",
            "‚úÖ Extracted: cgt_project_with_hakorn.zip\n",
            "\n",
            "üìã Checking H-AKORN modules:\n",
            "‚úÖ /content/cgt_project_with_hakorn/src/cgt/geometry/lorentz_hardened.py\n",
            "‚úÖ /content/cgt_project_with_hakorn/src/cgt/models/hakorn/model.py\n",
            "‚úÖ /content/cgt_project_with_hakorn/src/cgt/models/hakorn/phase_dynamics.py\n",
            "‚úÖ /content/cgt_project_with_hakorn/src/cgt/models/hakorn/attention.py\n"
          ]
        }
      ],
      "source": [
        "# @title 1.1 Upload CGT Project with H-AKORN\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "!rm -rf /content/cgt_project_with_hakorn\n",
        "print('üì§ Upload cgt_project_with_hakorn.zip:')\n",
        "\n",
        "uploaded = files.upload()\n",
        "for f in uploaded:\n",
        "    if f.endswith('.zip'):\n",
        "        with zipfile.ZipFile(f, 'r') as z:\n",
        "            z.extractall('/content')\n",
        "        print(f'‚úÖ Extracted: {f}')\n",
        "        os.remove(f)\n",
        "\n",
        "# Verificar H-AKORN\n",
        "required = [\n",
        "    '/content/cgt_project_with_hakorn/src/cgt/geometry/lorentz_hardened.py',\n",
        "    '/content/cgt_project_with_hakorn/src/cgt/models/hakorn/model.py',\n",
        "    '/content/cgt_project_with_hakorn/src/cgt/models/hakorn/phase_dynamics.py',\n",
        "    '/content/cgt_project_with_hakorn/src/cgt/models/hakorn/attention.py',\n",
        "]\n",
        "\n",
        "print('\\nüìã Checking H-AKORN modules:')\n",
        "for p in required:\n",
        "    print(f\"{'‚úÖ' if os.path.exists(p) else '‚ùå MISSING:'} {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDmKqEYtngkj",
        "outputId": "17dc5918-f1cd-4e1f-8131-954168aea674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ M√≥dulos limpos. Continue com as pr√≥ximas c√©lulas.\n"
          ]
        }
      ],
      "source": [
        "# @title 1.2 Install Dependencies\n",
        "!pip install -q transformers datasets tqdm POT torch\n",
        "# @title 0. Fix PyTorch (executar primeiro)\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Limpar m√≥dulos torch parcialmente carregados\n",
        "mods_to_remove = [k for k in sys.modules if 'torch' in k]\n",
        "for m in mods_to_remove:\n",
        "    del sys.modules[m]\n",
        "\n",
        "print(\"‚úÖ M√≥dulos limpos. Continue com as pr√≥ximas c√©lulas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I66bDEhpngkk",
        "outputId": "8449f188-33fe-4bf3-8118-88bc03a13861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Device: cuda\n",
            "   GPU: NVIDIA A100-SXM4-80GB\n",
            "   VRAM: 85.2 GB\n",
            "PyTorch: 2.9.0+cu126\n",
            "Device: cuda\n",
            "\n",
            "‚úÖ H-AKORN modules imported!\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Setup & Imports\n",
        "import sys\n",
        "sys.path.insert(0, '/content/cgt_project_with_hakorn/src')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, asdict\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# H-AKORN imports\n",
        "from cgt.geometry.lorentz_hardened import LorentzSubstrateHardened\n",
        "from cgt.models.hakorn import HAKORNTransformer, HAKORNLoss\n",
        "from cgt.losses.hyperbolic_lm_losses import (\n",
        "    HyperbolicLMLoss,\n",
        "    TeacherDistillationLoss,\n",
        "    RadiusRegularization,\n",
        "    ManifoldFidelityLoss,\n",
        ")\n",
        "\n",
        "# GPT-2 Tokenizer\n",
        "from transformers import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'üîß Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'   VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "\n",
        "OUTPUT_DIR = Path('/content/outputs/h_llm')\n",
        "CHECKPOINT_DIR = OUTPUT_DIR / 'checkpoints'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('\\n‚úÖ H-AKORN modules imported!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "27ZOLZnFngkk",
        "outputId": "1620633f-f850-4ec9-deee-99c6169475bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading GPT-2 tokenizer...\n",
            "‚úÖ Tokenizer loaded\n",
            "   Vocab size: 50257\n",
            "   EOS token: <|endoftext|> (id=50256)\n",
            "   PAD token: <|endoftext|> (id=50256)\n",
            "\n",
            "   Test: 'Hello, I am a hyperbolic language model!'\n",
            "   Tokens: [15496, 11, 314, 716, 257, 8718, 65, 4160, 3303, 2746, 0]\n",
            "   Decoded: 'Hello, I am a hyperbolic language model!'\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Load GPT-2 Tokenizer\n",
        "print(\"üîÑ Loading GPT-2 tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Tokenizer loaded\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   EOS token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})\")\n",
        "print(f\"   PAD token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})\")\n",
        "\n",
        "# Test\n",
        "test_text = \"Hello, I am a hyperbolic language model!\"\n",
        "tokens = tokenizer.encode(test_text)\n",
        "decoded = tokenizer.decode(tokens)\n",
        "print(f\"\\n   Test: '{test_text}'\")\n",
        "print(f\"   Tokens: {tokens}\")\n",
        "print(f\"   Decoded: '{decoded}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0EECkWOngkk",
        "outputId": "5bc3b485-02f4-4039-c74a-6cfc839fe25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è H-AKORN H-LLM Configuration:\n",
            "   Model: d=512, L=6, H=8\n",
            "   H-AKORN: coupling=1.5, Œ∫=-1.0\n",
            "   Phase modulation: True\n",
            "   Training: batch=16, lr=0.0001, epochs=10\n",
            "   Output: /content/outputs/h_akorn_llm\n"
          ]
        }
      ],
      "source": [
        "# @title 4. Configuration\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    # Model Architecture\n",
        "    vocab_size: int = 50257       # GPT-2 vocab\n",
        "    d_model: int = 512            # Embedding dimension\n",
        "    num_layers: int = 6           # Transformer layers\n",
        "    num_heads: int = 8            # Attention heads\n",
        "    d_ff: int = 2048              # FFN dimension\n",
        "    max_seq_len: int = 128        # Context window\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # H-AKORN Specific\n",
        "    curvature: float = -1.0              # Hyperbolic curvature (K < 0)\n",
        "    coupling_strength: float = 1.5       # Kuramoto coupling strength\n",
        "    use_phase_modulation: bool = True    # Phase modulation in attention\n",
        "\n",
        "    # Data\n",
        "    seq_length: int = 128\n",
        "\n",
        "    # Loss Weights\n",
        "    lambda_sync: float = 0.1             # Synchronization loss\n",
        "    lambda_variance: float = 0.05        # Phase variance loss\n",
        "    lambda_infonce: float = 0.3          # InfoNCE contrastive\n",
        "    lambda_manifold: float = 0.5         # Manifold fidelity\n",
        "    lambda_radius: float = 0.001         # Radius regularization\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 1e-4\n",
        "    num_epochs: int = 10\n",
        "    warmup_steps: int = 500\n",
        "    gradient_clip: float = 1.0\n",
        "    early_stop_patience: int = 5\n",
        "\n",
        "    # Checkpointing\n",
        "    output_dir: str = '/content/outputs/h_akorn_llm'\n",
        "    checkpoint_every: int = 500\n",
        "    eval_every: int = 100\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "print(\"‚öôÔ∏è H-AKORN H-LLM Configuration:\")\n",
        "print(f\"   Model: d={config.d_model}, L={config.num_layers}, H={config.num_heads}\")\n",
        "print(f\"   H-AKORN: coupling={config.coupling_strength}, Œ∫={config.curvature}\")\n",
        "print(f\"   Phase modulation: {config.use_phase_modulation}\")\n",
        "print(f\"   Training: batch={config.batch_size}, lr={config.learning_rate}, epochs={config.num_epochs}\")\n",
        "print(f\"   Output: {config.output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVwanKp-ngkl",
        "outputId": "cfbd612d-0cbb-41f6-f40f-940c57859bde",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading WikiText-2...\n",
            "‚ö†Ô∏è Failed to load WikiText-2 from Hugging Face\n",
            "   Reason: HfHubHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/datasets/wikitext/revision/b08601e04326c79dfdd32d625aee71d232d685c3 (Request ID: Root=1-697969e5-7af5e3a5412750d4619dea5a;06491b38-93fb-4dfd-8107-24ebf7c03eb7)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "‚ö†Ô∏è Falling back to synthetic dataset\n",
            "üß™ Using Synthetic Dataset\n",
            "‚úÖ DataLoaders ready\n",
            "   Train batches: 6250\n",
            "   Val batches: 625\n"
          ]
        }
      ],
      "source": [
        "# @title 5. Load Dataset (ROBUSTO, SEM HF DEPENDENCY)\n",
        "print(\"üîÑ Loading WikiText-2...\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Try Hugging Face WikiText (may fail due to HF issue)\n",
        "try:\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "    print(\"‚úÖ WikiText-2 loaded from Hugging Face\")\n",
        "    use_synthetic = False\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Failed to load WikiText-2 from Hugging Face\")\n",
        "    print(f\"   Reason: {type(e).__name__}: {e}\")\n",
        "    print(\"‚ö†Ô∏è Falling back to synthetic dataset\")\n",
        "    use_synthetic = True\n",
        "\n",
        "\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_length: int):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        all_text = \" \".join(t for t in texts if t and t.strip())\n",
        "        enc = tokenizer(\n",
        "            all_text,\n",
        "            return_attention_mask=False,\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "\n",
        "        self.tokens = enc[\"input_ids\"]\n",
        "        self.num_chunks = len(self.tokens) // seq_length\n",
        "        self.tokens = self.tokens[: self.num_chunks * seq_length]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_chunks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.seq_length\n",
        "        end = start + self.seq_length\n",
        "        tokens = torch.tensor(self.tokens[start:end], dtype=torch.long)\n",
        "        return {\"input_ids\": tokens, \"labels\": tokens}\n",
        "\n",
        "\n",
        "class SyntheticDataset(Dataset):\n",
        "    def __init__(self, vocab_size: int, seq_length: int, n_samples: int = 100_000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_length = seq_length\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = torch.randint(\n",
        "            0,\n",
        "            self.vocab_size,\n",
        "            (self.seq_length,),\n",
        "            dtype=torch.long,\n",
        "        )\n",
        "        return {\"input_ids\": tokens, \"labels\": tokens}\n",
        "\n",
        "\n",
        "# Build datasets\n",
        "if not use_synthetic:\n",
        "    print(\"üìò Using WikiText-2\")\n",
        "\n",
        "    train_dataset = WikiTextDataset(\n",
        "        dataset[\"train\"][\"text\"],\n",
        "        tokenizer,\n",
        "        config.max_seq_len,   # ‚úÖ CORRIGIDO: max_seq_len\n",
        "    )\n",
        "\n",
        "    val_dataset = WikiTextDataset(\n",
        "        dataset[\"validation\"][\"text\"],\n",
        "        tokenizer,\n",
        "        config.max_seq_len,   # ‚úÖ CORRIGIDO: max_seq_len\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print(\"üß™ Using Synthetic Dataset\")\n",
        "\n",
        "    train_dataset = SyntheticDataset(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        seq_length=config.max_seq_len,   # ‚úÖ CORRIGIDO: max_seq_len\n",
        "        n_samples=100_000,\n",
        "    )\n",
        "\n",
        "    val_dataset = SyntheticDataset(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        seq_length=config.max_seq_len,   # ‚úÖ CORRIGIDO: max_seq_len\n",
        "        n_samples=10_000,\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(  # ‚úÖ CORRIGIDO: train_dataloader (n√£o train_loader)\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(  # ‚úÖ CORRIGIDO: val_dataloader (n√£o val_loader)\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ DataLoaders ready\")\n",
        "print(f\"   Train batches: {len(train_dataloader)}\")\n",
        "print(f\"   Val batches: {len(val_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NJzy12Engkl",
        "outputId": "78efad17-e2df-4230-dec8-14b4034f76d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Creating H-AKORN Hyperbolic LLM...\n",
            "   Substrate: dim=512, K=1.0\n",
            "‚úÖ H-AKORN H-LLM created (with FIXED coupling.py)\n",
            "   Parameters: 18,916,808\n"
          ]
        }
      ],
      "source": [
        "# @title 6. Create H-AKORN H-LLM Model (RE-RUN)\n",
        "from cgt.geometry.lorentz_hardened import LorentzSubstrateHardened, LorentzConfig\n",
        "\n",
        "print(\"üîÑ Creating H-AKORN Hyperbolic LLM...\")\n",
        "\n",
        "# Create LorentzConfig\n",
        "lorentz_config = LorentzConfig(\n",
        "    intrinsic_dim=config.d_model,\n",
        "    initial_curvature=abs(1.0 / config.curvature),\n",
        "    learnable_curvature=False,\n",
        "    eps=1e-6,\n",
        ")\n",
        "\n",
        "# Create substrate\n",
        "substrate = LorentzSubstrateHardened(config=lorentz_config)\n",
        "\n",
        "# Add distance() method wrapper\n",
        "def distance_wrapper(self, x, y):\n",
        "    return self.dist(x, y)\n",
        "LorentzSubstrateHardened.distance = distance_wrapper\n",
        "\n",
        "print(f\"   Substrate: dim={lorentz_config.intrinsic_dim}, K={lorentz_config.initial_curvature:.1f}\")\n",
        "\n",
        "# Create H-AKORN Transformer (com coupling.py CORRIGIDO)\n",
        "model = HAKORNTransformer(\n",
        "    vocab_size=config.vocab_size,\n",
        "    d_model=config.d_model,\n",
        "    num_layers=config.num_layers,\n",
        "    num_heads=config.num_heads,\n",
        "    d_ff=config.d_ff,\n",
        "    dropout=config.dropout,\n",
        "    substrate=substrate,\n",
        "    coupling_strength=config.coupling_strength,\n",
        "    use_phase_modulation=config.use_phase_modulation,\n",
        ").to(device)\n",
        "\n",
        "# Create losses\n",
        "hakorn_loss = HAKORNLoss(\n",
        "    lambda_sync=config.lambda_sync,\n",
        "    lambda_variance=config.lambda_variance,\n",
        ")\n",
        "\n",
        "hyperbolic_loss = HyperbolicLMLoss(\n",
        "    substrate=substrate,\n",
        "    vocab_size=config.vocab_size,\n",
        "    lambda_lm=1.0,\n",
        "    lambda_infonce=config.lambda_infonce,\n",
        "    lambda_f1=config.lambda_manifold,\n",
        "    lambda_radius=config.lambda_radius,\n",
        ")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "print(f\"‚úÖ H-AKORN H-LLM created (with FIXED coupling.py)\")\n",
        "print(f\"   Parameters: {model.get_num_params():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzTgRLYwngkl",
        "outputId": "dbd955f8-8e64-435b-fbb4-9b184b0bf86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ H-AKORN Trainer class defined\n"
          ]
        }
      ],
      "source": [
        "# @title 7. H-AKORN Trainer Class\n",
        "class HAKORNLLMTrainer:\n",
        "    \"\"\"H-AKORN H-LLM Trainer with Kuramoto dynamics, checkpointing, early stopping.\"\"\"\n",
        "\n",
        "    def __init__(self, model, config, tokenizer):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "        self.optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        # Criar diret√≥rios\n",
        "        Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        Path(f\"{config.output_dir}/checkpoints\").mkdir(exist_ok=True)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Single training step with H-AKORN losses.\"\"\"\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(self.device)\n",
        "        labels = batch['input_ids'].to(self.device)\n",
        "\n",
        "        # Forward (H-AKORN returns: logits, loss, order_params, hidden_states)\n",
        "        output = self.model(input_ids, labels=labels, return_dict=True)\n",
        "\n",
        "        # H-AKORN loss (synchronization + variance)\n",
        "        hakorn_loss_dict = hakorn_loss(\n",
        "            output['loss'],\n",
        "            output['all_order_params'],\n",
        "        )\n",
        "\n",
        "        # Hyperbolic loss (manifold + radius + InfoNCE)\n",
        "        hyp_loss_dict = hyperbolic_loss(\n",
        "            output['logits'],\n",
        "            labels,\n",
        "            output['hidden_states'],\n",
        "            return_components=True,\n",
        "        )\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = hakorn_loss_dict['total'] + hyp_loss_dict['total']\n",
        "\n",
        "        # Backward\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.global_step += 1\n",
        "\n",
        "        # Metrics\n",
        "        metrics = {\n",
        "            'loss': total_loss.item(),\n",
        "            'lm_loss': output['loss'].item(),\n",
        "            'order_param': output['all_order_params'][-1].mean().item(),\n",
        "            'sync_loss': hakorn_loss_dict.get('sync_loss', 0),\n",
        "            'var_loss': hakorn_loss_dict.get('var_loss', 0),\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_step(self, dataloader):\n",
        "        \"\"\"Evaluation on validation set.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_order = 0\n",
        "        total_steps = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            output = self.model(input_ids, labels=input_ids, return_dict=True)\n",
        "\n",
        "            total_loss += output['loss'].item()\n",
        "            total_order += output['all_order_params'][-1].mean().item()\n",
        "            total_steps += 1\n",
        "\n",
        "        return {\n",
        "            'val_loss': total_loss / total_steps,\n",
        "            'val_order': total_order / total_steps,\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, val_loss):\n",
        "        \"\"\"Save checkpoint.\"\"\"\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'global_step': self.global_step,\n",
        "            'val_loss': val_loss,\n",
        "            'config': self.config,\n",
        "        }\n",
        "\n",
        "        path = f\"{self.config.output_dir}/checkpoints/checkpoint_step{self.global_step}.pt\"\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "        # Save best\n",
        "        if val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = val_loss\n",
        "            best_path = f\"{self.config.output_dir}/best_model.pt\"\n",
        "            torch.save(checkpoint, best_path)\n",
        "            print(f\"üíæ New best! val_loss={val_loss:.4f}\")\n",
        "            self.patience_counter = 0\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs):\n",
        "        \"\"\"Full training loop with early stopping.\"\"\"\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        order_params = []\n",
        "\n",
        "        print(f\"üöÄ Training H-AKORN H-LLM for {num_epochs} epochs...\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nüìç Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            # Training\n",
        "            epoch_metrics = []\n",
        "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "            for batch in pbar:\n",
        "                metrics = self.train_step(batch)\n",
        "                epoch_metrics.append(metrics)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{metrics['loss']:.3f}\",\n",
        "                    'r': f\"{metrics['order_param']:.2f}\",\n",
        "                })\n",
        "\n",
        "                # Eval\n",
        "                if self.global_step % self.config.eval_every == 0:\n",
        "                    val_metrics = self.eval_step(val_loader)\n",
        "                    print(f\"\\n‚úì Step {self.global_step}: val_loss={val_metrics['val_loss']:.4f}, r={val_metrics['val_order']:.3f}\")\n",
        "\n",
        "                    # Checkpoint\n",
        "                    if self.global_step % self.config.checkpoint_every == 0:\n",
        "                        self.save_checkpoint(val_metrics['val_loss'])\n",
        "                        val_losses.append((self.global_step, val_metrics['val_loss']))\n",
        "\n",
        "            # Epoch summary\n",
        "            avg_loss = np.mean([m['loss'] for m in epoch_metrics])\n",
        "            avg_order = np.mean([m['order_param'] for m in epoch_metrics])\n",
        "            train_losses.append(avg_loss)\n",
        "            order_params.append(avg_order)\n",
        "\n",
        "            print(f\"üìä Epoch {epoch+1}: loss={avg_loss:.4f}, order={avg_order:.3f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.config.early_stop_patience:\n",
        "                print(f\"\\n‚ö†Ô∏è Early stopping (patience={self.config.early_stop_patience})\")\n",
        "                break\n",
        "\n",
        "        return train_losses, val_losses, order_params\n",
        "\n",
        "print(\"‚úÖ H-AKORN Trainer class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "nP_Vg5Yongkm"
      },
      "outputs": [],
      "source": [
        "# @title 8. Create Trainer & Train\n",
        "# trainer = HAKORNLLMTrainer(model, config, tokenizer)\n",
        "\n",
        "# print(\"üöÄ H-AKORN H-LLM Trainer created\")\n",
        "# print(f\"   Model: {model.get_num_params():,} parameters\")\n",
        "# print(f\"   Dataset: WikiText-2\")\n",
        "# print(f\"   Epochs: {config.num_epochs}\")\n",
        "# print(f\"   Learning rate: {config.learning_rate}\")\n",
        "# print(f\"   Kuramoto coupling: {config.coupling_strength}\")\n",
        "# print(f\"   Output: {config.output_dir}\")\n",
        "# print()\n",
        "\n",
        "# # Start training\n",
        "# train_hist, val_hist, order_hist = trainer.train(\n",
        "#     train_dataloader,\n",
        "#     val_dataloader,\n",
        "#     num_epochs=config.num_epochs,\n",
        "# )\n",
        "\n",
        "# print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e1V2C8vXhKZj"
      },
      "outputs": [],
      "source": [
        "# @title DEBUG: Check tensor shapes\n",
        "# print(\"üîç Debugging tensor shapes...\")\n",
        "\n",
        "# # Create dummy input\n",
        "# dummy_input = torch.randint(0, config.vocab_size, (2, 128)).to(device)\n",
        "\n",
        "# try:\n",
        "#     with torch.no_grad():\n",
        "#         output = model(dummy_input)\n",
        "#     print(\"‚úÖ Forward pass OK!\")\n",
        "# except RuntimeError as e:\n",
        "#     print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "#     # Print model structure\n",
        "#     print(\"\\nüìã Model config:\")\n",
        "#     print(f\"   d_model: {config.d_model}\")\n",
        "#     print(f\"   num_heads: {config.num_heads}\")\n",
        "#     print(f\"   batch_size: {config.batch_size}\")\n",
        "#     print(f\"   max_seq_len: {config.max_seq_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUJFKI1bngkm"
      },
      "outputs": [],
      "source": [
        "# @title 9. Train (Early Stopping decides when to stop)\n",
        "#train_hist, val_hist = trainer.train(train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl476Ufbngkm",
        "outputId": "c113e59a-47ae-4d69-e425-a1bf7d8ebd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Nenhum hist√≥rico de treino. Execute a c√©lula de treinamento primeiro.\n"
          ]
        }
      ],
      "source": [
        "# @title 10. Training Analysis Plots\n",
        "def plot_analysis(th, vh, path):\n",
        "    fig, ax = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('H-LLM Training Analysis (GPT-2 Tokenizer)', fontsize=14, fontweight='bold')\n",
        "\n",
        "    s = [h['step'] for h in th]\n",
        "    loss = [h['loss'] for h in th]\n",
        "    ppl = [h['ppl'] for h in th]\n",
        "    r = [h['radius_mean'] for h in th]\n",
        "    rs = [h['radius_std'] for h in th]\n",
        "    viol = [h['mean_violation'] for h in th]\n",
        "    lr = [h['lr'] for h in th]\n",
        "    vs = [h['step'] for h in vh]\n",
        "    vl = [h['val_loss'] for h in vh]\n",
        "    vp = [h['val_ppl'] for h in vh]\n",
        "\n",
        "    # Loss\n",
        "    ax[0,0].plot(s, loss, alpha=0.3, label='Train')\n",
        "    w = min(50, len(loss)//10) if len(loss) > 10 else 1\n",
        "    if w > 1:\n",
        "        ax[0,0].plot(s[w-1:], np.convolve(loss, np.ones(w)/w, 'valid'), lw=2, label=f'MA{w}')\n",
        "    ax[0,0].plot(vs, vl, 'ro-', ms=4, label='Val')\n",
        "    ax[0,0].set_xlabel('Step'); ax[0,0].set_ylabel('Loss')\n",
        "    ax[0,0].set_title('Loss'); ax[0,0].legend(); ax[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Perplexity\n",
        "    ax[0,1].plot(vs, vp, 'g^-', ms=6)\n",
        "    ax[0,1].set_xlabel('Step'); ax[0,1].set_ylabel('PPL')\n",
        "    ax[0,1].set_title('Validation Perplexity'); ax[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Radius\n",
        "    ax[0,2].plot(s, r, label='Mean')\n",
        "    ax[0,2].fill_between(s, [x-y for x,y in zip(r,rs)], [x+y for x,y in zip(r,rs)], alpha=0.3)\n",
        "    ax[0,2].axhline(5.0, color='r', ls='--', label='R_max')\n",
        "    ax[0,2].set_xlabel('Step'); ax[0,2].set_ylabel('Radius')\n",
        "    ax[0,2].set_title('Embedding Radius'); ax[0,2].legend(); ax[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Manifold fidelity\n",
        "    ax[1,0].semilogy(s, viol)\n",
        "    ax[1,0].axhline(1e-5, color='g', ls='--', label='Target')\n",
        "    ax[1,0].set_xlabel('Step'); ax[1,0].set_ylabel('Violation')\n",
        "    ax[1,0].set_title('Manifold Fidelity'); ax[1,0].legend(); ax[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning Rate\n",
        "    ax[1,1].plot(s, lr)\n",
        "    ax[1,1].set_xlabel('Step'); ax[1,1].set_ylabel('LR')\n",
        "    ax[1,1].set_title('Learning Rate'); ax[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Train PPL\n",
        "    ax[1,2].semilogy(s, ppl, alpha=0.3)\n",
        "    if w > 1:\n",
        "        ax[1,2].semilogy(s[w-1:], np.convolve(ppl, np.ones(w)/w, 'valid'), lw=2)\n",
        "    ax[1,2].set_xlabel('Step'); ax[1,2].set_ylabel('PPL')\n",
        "    ax[1,2].set_title('Train Perplexity'); ax[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"üìä Saved: {path}\")\n",
        "\n",
        "if 'train_hist' in dir() and train_hist:\n",
        "    plot_analysis(train_hist, val_hist, OUTPUT_DIR / 'training_analysis.png')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Nenhum hist√≥rico de treino. Execute a c√©lula de treinamento primeiro.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPUurNCmngkm"
      },
      "source": [
        "## üéì Distillation: GPT-2 ‚Üí H-LLM\n",
        "\n",
        "Transferir conhecimento do GPT-2 pr√©-treinado para o modelo hiperb√≥lico.\n",
        "\n",
        "**Benef√≠cios:**\n",
        "- Converge muito mais r√°pido\n",
        "- Aprende distribui√ß√µes mais suaves\n",
        "- Melhor generaliza√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWkzsgHpngkn",
        "outputId": "b7473495-b6ef-4bba-893f-283e2a8555c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading GPT-2 Teacher...\n",
            "‚úÖ Teacher output shape: torch.Size([1, 32, 50257])\n",
            "   Hidden shape: torch.Size([1, 32, 768])\n"
          ]
        }
      ],
      "source": [
        "# @title 11. Load GPT-2 Teacher\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# IMPORTS CORRETOS ‚Äî sempre pelo pacote\n",
        "from cgt.distillation import (\n",
        "    GPT2TeacherWrapper,\n",
        "    DistillationConfig,\n",
        "    DistillationTrainer,\n",
        "    plot_distillation_analysis,\n",
        ")\n",
        "\n",
        "print(\"üîÑ Loading GPT-2 Teacher...\")\n",
        "teacher = GPT2TeacherWrapper(model_name=\"gpt2\", device=DEVICE)\n",
        "\n",
        "# Teste r√°pido do teacher\n",
        "with torch.no_grad():\n",
        "    test_ids = torch.randint(0, 50257, (1, 32), device=DEVICE)\n",
        "    t_out = teacher(test_ids)\n",
        "    print(f\"‚úÖ Teacher output shape: {t_out['logits'].shape}\")\n",
        "    print(f\"   Hidden shape: {t_out['hidden_states'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WgRnTNiQngkn",
        "outputId": "343e6bec-3c4d-44b3-cd0f-cc072cedd161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distillation Config:\n",
            "  alpha: 0.5\n",
            "  temperature: 2.0\n",
            "  lambda_distill: 0.7\n",
            "  learning_rate: 5e-05\n",
            "  weight_decay: 0.01\n",
            "  max_steps: 1000\n",
            "  warmup_steps: 300\n",
            "  gradient_clip: 1.0\n",
            "  early_stopping_patience: 1\n",
            "  early_stopping_min_delta: 0.01\n",
            "  checkpoint_every: 1000\n",
            "  eval_every: 200\n",
            "  log_every: 100\n",
            "  keep_last_n_checkpoints: 3\n"
          ]
        }
      ],
      "source": [
        "# @title 12. Distillation Configuration\n",
        "distill_config = DistillationConfig(\n",
        "    alpha=0.5,\n",
        "    temperature=2.0,\n",
        "    lambda_distill=0.7,\n",
        "\n",
        "    learning_rate=5e-5,\n",
        "    max_steps=1000,          # ‚¨ÖÔ∏è teto realista\n",
        "    warmup_steps=300,        # ‚¨ÖÔ∏è fase √∫til da distil\n",
        "\n",
        "    early_stopping_patience=1,  # ‚¨ÖÔ∏è 1 eval sem melhora = STOP\n",
        "    eval_every=200,\n",
        "    log_every=100,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Distillation Config:\")\n",
        "for k, v in asdict(distill_config).items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55vsjfjXngkn",
        "outputId": "6254afe4-345e-4183-b80f-24a3c50026f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fresh distillation (no checkpoint found)\n",
            "\n",
            "============================================================\n",
            "üìä TRAINER STATUS\n",
            "============================================================\n",
            "   Current step: 0\n",
            "   Max steps: 1000\n",
            "   Remaining: 1000\n",
            "   Device: cuda\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# @title 13. Create Distillation Trainer & Resume\n",
        "distill_trainer = DistillationTrainer(\n",
        "    student=model,\n",
        "    teacher=teacher,\n",
        "    config=distill_config,\n",
        "    tokenizer=tokenizer,\n",
        "    checkpoint_dir=CHECKPOINT_DIR,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "# Check for migrated checkpoint first, then old checkpoint\n",
        "migrated_ckpt = CHECKPOINT_DIR / \"distill_latest_migrated.pt\"\n",
        "old_ckpt = CHECKPOINT_DIR / \"distill_latest.pt\"\n",
        "\n",
        "if migrated_ckpt.exists():\n",
        "    print(f\"‚úÖ Found migrated checkpoint: {migrated_ckpt}\")\n",
        "    resume = input(\"Resume from migrated checkpoint? (y/n): \").strip().lower()\n",
        "    if resume == \"y\":\n",
        "        distill_trainer.load(str(migrated_ckpt))\n",
        "        print(f\"‚úÖ Loaded migrated checkpoint at step {distill_trainer.step}\")\n",
        "\n",
        "        # Fix max_steps if needed\n",
        "        if distill_trainer.step >= distill_config.max_steps:\n",
        "            new_max_steps = max(20000, distill_trainer.step + 1000)\n",
        "            print(f\"\\n‚ö†Ô∏è  Step {distill_trainer.step} >= max_steps {distill_config.max_steps}\")\n",
        "            print(f\"‚úÖ Auto-fixing: max_steps ‚Üí {new_max_steps}\")\n",
        "            distill_config.max_steps = new_max_steps\n",
        "            distill_trainer.config.max_steps = new_max_steps\n",
        "\n",
        "        print(f\"   Remaining steps: {distill_config.max_steps - distill_trainer.step}\")\n",
        "    else:\n",
        "        print(\"Starting fresh distillation\")\n",
        "\n",
        "elif old_ckpt.exists():\n",
        "    print(f\"‚ö†Ô∏è  Found OLD checkpoint (needs migration): {old_ckpt}\")\n",
        "    migrate = input(\"Migrate now? (y/n): \").strip().lower()\n",
        "\n",
        "    if migrate == \"y\":\n",
        "        print(\"\\nüîÑ Migrating checkpoint...\")\n",
        "        import torch\n",
        "        ckpt = torch.load(old_ckpt, map_location='cpu')\n",
        "        old_state = ckpt['model']\n",
        "        new_state = {}\n",
        "        migrated_count = 0\n",
        "\n",
        "        for k, v in old_state.items():\n",
        "            if '.norm1.weight' in k:\n",
        "                new_state[k.replace('.norm1.weight', '.norm1.layer_norm.weight')] = v\n",
        "                migrated_count += 1\n",
        "            elif '.norm1.bias' in k:\n",
        "                new_state[k.replace('.norm1.bias', '.norm1.layer_norm.bias')] = v\n",
        "                migrated_count += 1\n",
        "            elif '.norm2.weight' in k:\n",
        "                new_state[k.replace('.norm2.weight', '.norm2.layer_norm.weight')] = v\n",
        "                migrated_count += 1\n",
        "            elif '.norm2.bias' in k:\n",
        "                new_state[k.replace('.norm2.bias', '.norm2.layer_norm.bias')] = v\n",
        "                migrated_count += 1\n",
        "            elif 'final_norm.weight' in k:\n",
        "                new_state[k.replace('final_norm.weight', 'final_norm.layer_norm.weight')] = v\n",
        "                migrated_count += 1\n",
        "            elif 'final_norm.bias' in k:\n",
        "                new_state[k.replace('final_norm.bias', 'final_norm.layer_norm.bias')] = v\n",
        "                migrated_count += 1\n",
        "            else:\n",
        "                new_state[k] = v\n",
        "\n",
        "        ckpt['model'] = new_state\n",
        "        torch.save(ckpt, migrated_ckpt)\n",
        "        print(f\"‚úÖ Migrated {migrated_count} keys ‚Üí {migrated_ckpt}\")\n",
        "\n",
        "        # Load migrated checkpoint\n",
        "        distill_trainer.load(str(migrated_ckpt))\n",
        "        print(f\"‚úÖ Loaded migrated checkpoint at step {distill_trainer.step}\")\n",
        "\n",
        "        # Fix max_steps if needed\n",
        "        if distill_trainer.step >= distill_config.max_steps:\n",
        "            new_max_steps = max(20000, distill_trainer.step + 1000)\n",
        "            print(f\"\\n‚ö†Ô∏è  Step {distill_trainer.step} >= max_steps {distill_config.max_steps}\")\n",
        "            print(f\"‚úÖ Auto-fixing: max_steps ‚Üí {new_max_steps}\")\n",
        "            distill_config.max_steps = new_max_steps\n",
        "            distill_trainer.config.max_steps = new_max_steps\n",
        "\n",
        "        print(f\"   Remaining steps: {distill_config.max_steps - distill_trainer.step}\")\n",
        "    else:\n",
        "        print(\"Starting fresh distillation\")\n",
        "\n",
        "else:\n",
        "    print(\"Starting fresh distillation (no checkpoint found)\")\n",
        "\n",
        "# Final validation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä TRAINER STATUS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   Current step: {distill_trainer.step}\")\n",
        "print(f\"   Max steps: {distill_config.max_steps}\")\n",
        "print(f\"   Remaining: {distill_config.max_steps - distill_trainer.step}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üö® EMERGENCY MEMORY FIX - Run NOW\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üö® EMERGENCY MEMORY CLEANUP\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. AGGRESSIVE CACHE CLEARING\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.synchronize()\n",
        "print(\"‚úÖ Cache cleared\")\n",
        "\n",
        "# 2. CHECK MEMORY STATUS\n",
        "if torch.cuda.is_available():\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    free = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9\n",
        "\n",
        "    print(f\"\\nüìä GPU Memory:\")\n",
        "    print(f\"   Total: {total:.1f} GB\")\n",
        "    print(f\"   Allocated: {allocated:.1f} GB ({allocated/total*100:.1f}%)\")\n",
        "    print(f\"   Reserved: {reserved:.1f} GB\")\n",
        "    print(f\"   Free: {free:.1f} GB ({free/total*100:.1f}%)\")\n",
        "\n",
        "    if allocated / total > 0.95:\n",
        "        print(f\"\\n‚ùå CRITICAL: GPU is {allocated/total*100:.1f}% FULL!\")\n",
        "        print(f\"   You MUST reduce batch size or model size\")\n",
        "    elif free < 1.0:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: Only {free:.1f} GB free\")\n",
        "        print(f\"   Recommend batch_size=1\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ OK: {free:.1f} GB available\")\n",
        "\n",
        "# 3. SET MINIMAL BATCH SIZE\n",
        "BATCH_SIZE = 1  # CRITICAL: Must be 1\n",
        "print(f\"\\n‚ö° Setting BATCH_SIZE = {BATCH_SIZE}\")\n",
        "\n",
        "# 4. RECREATE DATALOADERS\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "if 'collate_fn' not in globals():\n",
        "    collate_fn = None\n",
        "\n",
        "if 'train_dataset' in globals() and 'val_dataset' in globals():\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=True,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Dataloaders recreated with batch_size={BATCH_SIZE}\")\n",
        "    print(f\"   Train: {len(train_dataloader)} batches\")\n",
        "    print(f\"   Val: {len(val_dataloader)} batches\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Datasets not found - create them first\")\n",
        "\n",
        "# 5. REDUCE LM_HEAD CHUNK SIZES\n",
        "if 'model' in globals():\n",
        "    if hasattr(model, 'lm_head') and hasattr(model.lm_head, 'chunk_size'):\n",
        "        model.lm_head.chunk_size = 25  # VERY SMALL\n",
        "        if hasattr(model.lm_head, 'seq_chunk_size'):\n",
        "            model.lm_head.seq_chunk_size = 16  # VERY SMALL\n",
        "        print(f\"‚úÖ Reduced lm_head chunk_size to {model.lm_head.chunk_size}\")\n",
        "        if hasattr(model.lm_head, 'seq_chunk_size'):\n",
        "            print(f\"‚úÖ Reduced seq_chunk_size to {model.lm_head.seq_chunk_size}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  lm_head doesn't have adjustable chunk_size\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Model not found\")\n",
        "\n",
        "# 6. UPDATE CONFIG\n",
        "if 'distill_config' in globals():\n",
        "    distill_config.batch_size = BATCH_SIZE\n",
        "    print(f\"‚úÖ Updated distill_config.batch_size = {BATCH_SIZE}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚ö†Ô∏è  CRITICAL INSTRUCTIONS:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"1. GPU is nearly FULL - batch_size MUST be 1\")\n",
        "print(\"2. Training will be VERY SLOW with batch_size=1\")\n",
        "print(\"3. Consider:\")\n",
        "print(\"   ‚Ä¢ Reduce model size (d_model, num_layers)\")\n",
        "print(\"   ‚Ä¢ Reduce sequence length (max_position_embeddings)\")\n",
        "print(\"   ‚Ä¢ Remove teacher model if using distillation\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "cellView": "form",
        "id": "P7BsutgLOj8H",
        "outputId": "c831e8d9-d5ef-41c7-bd28-1ea46e6d8a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üö® EMERGENCY MEMORY CLEANUP\n",
            "======================================================================\n",
            "‚úÖ Cache cleared\n",
            "\n",
            "üìä GPU Memory:\n",
            "   Total: 85.2 GB\n",
            "   Allocated: 0.7 GB (0.8%)\n",
            "   Reserved: 84.6 GB\n",
            "   Free: 84.4 GB (99.2%)\n",
            "\n",
            "‚úÖ OK: 84.4 GB available\n",
            "\n",
            "‚ö° Setting BATCH_SIZE = 1\n",
            "‚úÖ Dataloaders recreated with batch_size=1\n",
            "   Train: 100000 batches\n",
            "   Val: 10000 batches\n",
            "‚úÖ Reduced lm_head chunk_size to 25\n",
            "‚úÖ Reduced seq_chunk_size to 16\n",
            "‚úÖ Updated distill_config.batch_size = 1\n",
            "\n",
            "======================================================================\n",
            "‚ö†Ô∏è  CRITICAL INSTRUCTIONS:\n",
            "======================================================================\n",
            "1. GPU is nearly FULL - batch_size MUST be 1\n",
            "2. Training will be VERY SLOW with batch_size=1\n",
            "3. Consider:\n",
            "   ‚Ä¢ Reduce model size (d_model, num_layers)\n",
            "   ‚Ä¢ Reduce sequence length (max_position_embeddings)\n",
            "   ‚Ä¢ Remove teacher model if using distillation\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-t5eV_Rngkn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 14. Run Distillation (Early Stopping decides)\n",
        "distill_train_hist, distill_val_hist = distill_trainer.train(train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 15. LM-only Configuration (POST-DISTILL)\n",
        "\n",
        "from dataclasses import asdict\n",
        "\n",
        "lm_config = DistillationConfig(\n",
        "    alpha=0.0,\n",
        "    lambda_distill=0.0,\n",
        "    temperature=1.0,\n",
        "\n",
        "    learning_rate=3e-5,\n",
        "    max_steps=3000,\n",
        "    warmup_steps=300,\n",
        "\n",
        "    early_stopping_patience=2,\n",
        "    eval_every=300,\n",
        "    log_every=100,\n",
        ")\n",
        "\n",
        "print(\"LM-only config DEFINIDO:\")\n",
        "for k, v in asdict(lm_config).items():\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nqL6AUwC_3y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 16. LM-only Checkpoint Directory\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "lm_checkpoint_dir = Path(\"/content/drive/MyDrive/H-LLM_Checkpoints/LM_ONLY\")\n",
        "lm_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"LM-only checkpoints:\", lm_checkpoint_dir)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NeV1GcJT_6xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 17. Trainer LM-only (NO TEACHER)\n",
        "\n",
        "lm_trainer = DistillationTrainer(\n",
        "    student=model,\n",
        "    teacher=None,          # N√ÉO PASSA TEACHER\n",
        "    config=lm_config,\n",
        "    tokenizer=tokenizer,\n",
        "    checkpoint_dir=lm_checkpoint_dir,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tNVxC1Xo_9il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 18. Run LM-only Training\n",
        "\n",
        "lm_train_hist, lm_val_hist = lm_trainer.train(\n",
        "    train_dataloader,\n",
        "    val_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "QVYtDEoAAKFB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Vmp7bZgngkn"
      },
      "outputs": [],
      "source": [
        "# @title 15. Distillation Analysis Plots\n",
        "if distill_train_hist:\n",
        "    plot_distillation_analysis(\n",
        "        distill_train_hist,\n",
        "        distill_val_hist,\n",
        "        OUTPUT_DIR / \"distillation_analysis.png\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W2mPLYtRngkn"
      },
      "outputs": [],
      "source": [
        "# @title 16. Load Best Distilled Model\n",
        "import math\n",
        "distill_best = CHECKPOINT_DIR / \"distill_best.pt\"\n",
        "if distill_best.exists():\n",
        "    ckpt = torch.load(distill_best, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    print(f\"‚úÖ Best distilled model loaded from step {ckpt['step']}\")\n",
        "    print(f\"   Val loss: {ckpt['best_val']:.4f}\")\n",
        "    print(f\"   Val PPL: {math.exp(min(ckpt['best_val'], 20)):.1f}\")\n",
        "else:\n",
        "    # Fallback to regular best\n",
        "    best = CHECKPOINT_DIR / \"best.pt\"\n",
        "    if best.exists():\n",
        "        ckpt = torch.load(best, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"‚úÖ Best model loaded from step {ckpt['step']}\")\n",
        "\n",
        "model.eval()\n",
        "print(\"\\nüîÆ Model ready for chat!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LXfPsNbyngkn"
      },
      "outputs": [],
      "source": [
        "# @title 17. üó£Ô∏è Chat Interativo com H-LLM (GERA√á√ÉO CORRETA)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "MUDAN√áAS:\n",
        "- Usa model.forward() para obter logits (consistente com treino)\n",
        "- Valida se lm_head √© hiperb√≥lico ou euclidiano\n",
        "- Remove tentativas de reconstruir logits manualmente\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# ============================================================================\n",
        "# CHAT CLASS\n",
        "# ============================================================================\n",
        "class HyperbolicChat:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.conversation = []\n",
        "        self.max_context = 512\n",
        "\n",
        "        # Validar modelo\n",
        "        self._validate_model()\n",
        "\n",
        "    def _validate_model(self):\n",
        "        \"\"\"Valida componentes hiperb√≥licos do modelo.\"\"\"\n",
        "        # Verificar substrate\n",
        "        has_substrate = hasattr(self.model, 'substrate') and self.model.substrate is not None\n",
        "\n",
        "        # Verificar lm_head\n",
        "        has_lm_head = hasattr(self.model, 'lm_head')\n",
        "\n",
        "        if not has_substrate:\n",
        "            print(\"‚ö†Ô∏è  AVISO: Modelo sem substrate - usando gera√ß√£o Euclidiana\")\n",
        "            self.use_hyperbolic = False\n",
        "            return\n",
        "\n",
        "        if not has_lm_head:\n",
        "            raise RuntimeError(\n",
        "                \"ERRO CR√çTICO: Modelo tem substrate mas n√£o tem lm_head.\"\n",
        "            )\n",
        "\n",
        "        # Verificar tipo de lm_head\n",
        "        lm_head = self.model.lm_head\n",
        "        is_hyperbolic_head = (\n",
        "            hasattr(lm_head, 'substrate') or\n",
        "            hasattr(lm_head, 'compute_logits_hyperbolic') or\n",
        "            'Hyperbolic' in type(lm_head).__name__\n",
        "        )\n",
        "\n",
        "        if not is_hyperbolic_head:\n",
        "            print(f\"‚ö†Ô∏è  AVISO: lm_head ({type(lm_head).__name__}) √© Euclidiano\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Geometria Hiperb√≥lica VALIDADA\")\n",
        "            print(f\"   Substrate: {type(self.model.substrate).__name__}\")\n",
        "            print(f\"   LM Head: {type(lm_head).__name__}\")\n",
        "\n",
        "        self.use_hyperbolic = True\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.conversation.append({\n",
        "            'role': role,\n",
        "            'content': content,\n",
        "            'time': datetime.now().strftime('%H:%M:%S')\n",
        "        })\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, max_tokens=100, temperature=0.8, top_k=50, top_p=0.95):\n",
        "        \"\"\"\n",
        "        Gera√ß√£o autoregressiva usando model.forward().\n",
        "\n",
        "        IMPORTANTE: Usa EXATAMENTE o mesmo caminho do treino.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Construir contexto da conversa\n",
        "        context = \"\"\n",
        "        for msg in self.conversation:\n",
        "            prefix = \"User: \" if msg['role'] == 'user' else \"Assistant: \"\n",
        "            context += prefix + msg['content'] + \"\\n\"\n",
        "        context += \"User: \" + prompt + \"\\nAssistant:\"\n",
        "\n",
        "        # Tokenizar\n",
        "        input_ids = self.tokenizer.encode(context, return_tensors='pt').to(self.device)\n",
        "\n",
        "        # Truncar se muito longo\n",
        "        if input_ids.shape[1] > self.max_context:\n",
        "            input_ids = input_ids[:, -self.max_context:]\n",
        "\n",
        "        # Gerar tokens\n",
        "        generated = []\n",
        "        for _ in range(max_tokens):\n",
        "            # Forward pass - MESMO CAMINHO DO TREINO\n",
        "            outputs = self.model(input_ids, return_dict=True)\n",
        "\n",
        "            # Logits v√™m do lm_head (hiperb√≥lico ou euclidiano)\n",
        "            logits = outputs['logits'][:, -1, :]\n",
        "\n",
        "            # Aplicar temperatura\n",
        "            if temperature != 1.0:\n",
        "                logits = logits / temperature\n",
        "\n",
        "            # Top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Top-p filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Stop conditions\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            generated.append(next_token.item())\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "            # Truncar\n",
        "            if input_ids.shape[1] > self.max_context:\n",
        "                input_ids = input_ids[:, -self.max_context:]\n",
        "\n",
        "            # Stop at newline\n",
        "            decoded = self.tokenizer.decode([next_token.item()])\n",
        "            if '\\n' in decoded and len(generated) > 10:\n",
        "                break\n",
        "\n",
        "        response = self.tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
        "        return response\n",
        "\n",
        "    def chat(self, user_input, **kwargs):\n",
        "        self.add_message('user', user_input)\n",
        "        response = self.generate(user_input, **kwargs)\n",
        "        self.add_message('assistant', response)\n",
        "        return response\n",
        "\n",
        "    def clear(self):\n",
        "        self.conversation = []\n",
        "\n",
        "    def get_html(self):\n",
        "        html = '''\n",
        "        <style>\n",
        "            .chat-box { max-height: 400px; overflow-y: auto; padding: 10px;\n",
        "                        border: 1px solid #ddd; border-radius: 10px; background: #f5f5f5; }\n",
        "            .msg { margin: 8px 0; padding: 10px 15px; border-radius: 15px; max-width: 85%; }\n",
        "            .user { background: #007bff; color: white; margin-left: auto; text-align: right; }\n",
        "            .assistant { background: #28a745; color: white; }\n",
        "            .role { font-size: 0.75em; font-weight: bold; margin-bottom: 3px; }\n",
        "            .time { font-size: 0.65em; opacity: 0.7; margin-top: 3px; }\n",
        "            .stats { font-size: 0.8em; color: #666; padding: 5px; margin-top: 10px;\n",
        "                     background: white; border-radius: 5px; }\n",
        "        </style>\n",
        "        <div class=\"chat-box\" id=\"chat-box\">\n",
        "        '''\n",
        "        for msg in self.conversation:\n",
        "            cls = msg['role']\n",
        "            icon = 'üë§' if cls == 'user' else 'üîÆ'\n",
        "            html += f'''\n",
        "            <div class=\"msg {cls}\">\n",
        "                <div class=\"role\">{icon} {cls.title()}</div>\n",
        "                <div>{msg['content']}</div>\n",
        "                <div class=\"time\">{msg['time']}</div>\n",
        "            </div>\n",
        "            '''\n",
        "        html += f'''\n",
        "        <div class=\"stats\">üí¨ {len(self.conversation)} messages | üîÆ H-LLM</div>\n",
        "        </div>\n",
        "        <script>document.getElementById('chat-box').scrollTop = document.getElementById('chat-box').scrollHeight;</script>\n",
        "        '''\n",
        "        return html\n",
        "\n",
        "# ============================================================================\n",
        "# INTERFACE\n",
        "# ============================================================================\n",
        "chat = HyperbolicChat(model, tokenizer, DEVICE)\n",
        "\n",
        "output = widgets.Output()\n",
        "input_box = widgets.Textarea(\n",
        "    placeholder='Digite sua mensagem...',\n",
        "    layout=widgets.Layout(width='100%', height='60px')\n",
        ")\n",
        "\n",
        "send_btn = widgets.Button(description='üì§ Enviar', button_style='primary')\n",
        "clear_btn = widgets.Button(description='üóëÔ∏è Limpar', button_style='warning')\n",
        "save_btn = widgets.Button(description='üíæ Salvar', button_style='info')\n",
        "\n",
        "temp_slider = widgets.FloatSlider(value=0.8, min=0.1, max=2.0, step=0.1, description='Temperatura:')\n",
        "tokens_slider = widgets.IntSlider(value=100, min=20, max=300, step=10, description='Max Tokens:')\n",
        "topk_slider = widgets.IntSlider(value=50, min=0, max=100, step=10, description='Top-K:')\n",
        "topp_slider = widgets.FloatSlider(value=0.95, min=0.5, max=1.0, step=0.05, description='Top-P:')\n",
        "\n",
        "def update():\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(chat.get_html()))\n",
        "\n",
        "def on_send(b):\n",
        "    text = input_box.value.strip()\n",
        "    if not text:\n",
        "        return\n",
        "    input_box.value = ''\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        display(HTML(chat.get_html() + '<p style=\"color:#888\">üîÑ Generating...</p>'))\n",
        "    chat.chat(text, max_tokens=tokens_slider.value, temperature=temp_slider.value,\n",
        "              top_k=topk_slider.value, top_p=topp_slider.value)\n",
        "    update()\n",
        "\n",
        "def on_clear(b):\n",
        "    chat.clear()\n",
        "    update()\n",
        "\n",
        "def on_save(b):\n",
        "    path = OUTPUT_DIR / f'chat_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(chat.conversation, f, indent=2)\n",
        "    print(f\"üíæ Saved: {path}\")\n",
        "\n",
        "send_btn.on_click(on_send)\n",
        "clear_btn.on_click(on_clear)\n",
        "save_btn.on_click(on_save)\n",
        "\n",
        "header = widgets.HTML('''\n",
        "<h2>üîÆ Chat com H-LLM (Hyperbolic Language Model)</h2>\n",
        "<p style=\"color:#666\">Gera√ß√£o via forward() oficial - Consistente com treino</p>\n",
        "''')\n",
        "\n",
        "params_box = widgets.VBox([\n",
        "    widgets.HTML('<b>‚öôÔ∏è Par√¢metros de Gera√ß√£o:</b>'),\n",
        "    widgets.HBox([temp_slider, tokens_slider]),\n",
        "    widgets.HBox([topk_slider, topp_slider]),\n",
        "])\n",
        "\n",
        "buttons_box = widgets.HBox([send_btn, clear_btn, save_btn])\n",
        "\n",
        "display(header)\n",
        "display(params_box)\n",
        "display(output)\n",
        "display(input_box)\n",
        "display(buttons_box)\n",
        "\n",
        "update()\n",
        "\n",
        "print('''\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë  üîÆ H-LLM Chat (CORRE√á√ÉO DEFINITIVA)                           ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë  ‚úÖ Gera√ß√£o usa model.forward() - MESMO CAMINHO DO TREINO     ‚ïë\n",
        "‚ïë  ‚úÖ Logits v√™m do lm_head oficial                             ‚ïë\n",
        "‚ïë  ‚úÖ SEM reconstru√ß√£o manual de dist√¢ncias                     ‚ïë\n",
        "‚ïë  ‚úÖ SEM try/except mascarando erros                           ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b2MWry66ngko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90604db1-d663-477b-8ab2-b50a856b2c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "üìÅ Files saved to: /content/drive/MyDrive/H-LLM_Checkpoints\n"
          ]
        }
      ],
      "source": [
        "# @title 18. Save to Google Drive\n",
        "from datetime import datetime\n",
        "\n",
        "SAVE_TO_DRIVE = True  # @param {type:\"boolean\"}\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    import shutil\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    dp = Path('/content/drive/MyDrive/H-LLM_Checkpoints')\n",
        "    dp.mkdir(parents=True, exist_ok=True)\n",
        "    ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    for f in ['best.pt', 'latest.pt']:\n",
        "        src = CHECKPOINT_DIR / f\n",
        "        if src.exists():\n",
        "            shutil.copy(src, dp / f'{f.replace(\".pt\", \"\")}_{ts}.pt')\n",
        "            print(f\"‚úÖ Saved {f}\")\n",
        "\n",
        "    plot = OUTPUT_DIR / 'training_analysis.png'\n",
        "    if plot.exists():\n",
        "        shutil.copy(plot, dp / f'analysis_{ts}.png')\n",
        "\n",
        "    print(f\"\\nüìÅ Files saved to: {dp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlIK4CVungko"
      },
      "source": [
        "## üìã Summary\n",
        "\n",
        "### ‚úÖ Features:\n",
        "- **GPT-2 Tokenizer** (50257 tokens reais)\n",
        "- **WikiText-2** dataset real\n",
        "- **üéì GPT-2 Distillation** - transfer√™ncia de conhecimento\n",
        "- **Checkpoints** autom√°ticos\n",
        "- **Early Stopping** inteligente\n",
        "- **Chat Interativo** com par√¢metros ajust√°veis\n",
        "\n",
        "### üîß Fluxo:\n",
        "1. C√©lulas 1-10: Setup + Treino b√°sico (opcional)\n",
        "2. C√©lulas 11-16: **Distillation GPT-2 ‚Üí H-LLM**\n",
        "3. C√©lula 17: Chat interativo\n",
        "\n",
        "### üí¨ Chat Par√¢metros:\n",
        "- **Temperatura**: Criatividade (0.1=conservador, 2.0=criativo)\n",
        "- **Max Tokens**: Tamanho m√°ximo da resposta\n",
        "- **Top-K**: Filtro de tokens mais prov√°veis\n",
        "- **Top-P**: Nucleus sampling"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}