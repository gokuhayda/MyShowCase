# SPDX-License-Identifier: CC-BY-NC-SA-4.0
# Copyright Â© 2026 Ã‰ric Gustavo Reis de Sena. All Rights Reserved.
# Patent Pending. For commercial licensing: eirikreisena@gmail.com

"""
Lorentz Substrate [HARDENED VERSION]
====================================

Exact implementation matching CGT_Paper_Ready_v6_1_HARDENED notebook.
All operations are metric-consistent and numerically stable.

Mathematical Status
-------------------
- All core operations: EXACT (closed-form Lorentz geometry)
- Numerical stability: HARDENED (eps clamps, safe_acosh)
- Device sync: AUTOMATIC

Author: Ã‰ric Gustavo Reis de Sena
Date: January 2026
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F


def safe_sqrt(x: torch.Tensor, eps: float = 1e-11) -> torch.Tensor:
    """Numerically stable square root."""
    return torch.sqrt(torch.clamp(x, min=eps))


def safe_acosh(x: torch.Tensor, eps: float = 1e-7) -> torch.Tensor:
    """
    Numerically stable inverse hyperbolic cosine with Taylor expansion.
    
    AUDIT FIX: The simple clamp creates a zero-gradient region for x âˆˆ [1, 1+Îµ].
    Taylor expansion of arccosh(1+Î´) â‰ˆ âˆš(2Î´) restores gradient flow near x=1.
    
    Mathematical basis:
        - Standard path: arccosh(x) for x > 1+Îµ (stable)
        - Taylor path: âˆš(2(x-1)) for x âˆˆ [1, 1+Îµ] (gradient-preserving)
    
    This enables learning of identity and fine refinement of positive pairs.
    """
    delta = x - 1.0
    mask = delta < eps
    
    # Standard path (stable far from 1)
    val_standard = torch.acosh(torch.clamp(x, min=1.0 + eps))
    
    # Taylor path: arccosh(1+z) â‰ˆ sqrt(2z) for small z
    # Clamp delta to avoid sqrt of negative
    val_taylor = torch.sqrt(2.0 * torch.clamp(delta, min=0.0) + 1e-15)
    
    return torch.where(mask, val_taylor, val_standard)


@dataclass
class LorentzConfig:
    """
    Configuration for the Lorentz (hyperbolic) manifold substrate.

    The Lorentz model embeds points on the upper sheet of a two-sheeted
    hyperboloid in Minkowski space with signature (-,+,+,...,+).

    Attributes:
        intrinsic_dim: Dimension of the hyperbolic space (n in H^n)
        eps: Numerical stability epsilon for clamp operations
        learnable_curvature: Whether K is a learnable parameter
        initial_curvature: Initial value of sectional curvature K
        curvature_min: Minimum allowed curvature (prevents collapse)
        curvature_max: Maximum allowed curvature (prevents explosion)
        
    Notes:
        - Ambient dimension is intrinsic_dim + 1 (for time component)
        - Points satisfy: -xâ‚€Â² + xâ‚Â² + ... + xâ‚™Â² = -1/K
    """
    intrinsic_dim: int = 32
    eps: float = 1e-6
    learnable_curvature: bool = True
    initial_curvature: float = 1.0
    curvature_min: float = 0.1
    curvature_max: float = 10.0


class LorentzSubstrateHardened(nn.Module):
    """
    Lorentz (Hyperboloid) Manifold Implementation [HARDENED].
    
    This is the production-ready substrate for CGT training, matching
    exactly the implementation in CGT_Paper_Ready_v6_1_HARDENED notebook.
    
    Features:
        - Metric-consistent similarity functions
        - GPU-optimized distance matrices
        - Riemannian gradient conversion
        - Automatic device synchronization
    
    Attributes:
        n: Intrinsic dimension (hyperbolic dimension)
        K: Sectional curvature parameter (learnable or fixed)
        eps: Numerical stability constant
        
    Notes:
        - Mathematical Identity: Lorentz (Hyperboloid) Model of H^n
        - Classification: EXACT (all operations are closed-form)
        - Stability: HARDENED (clamps, safe_acosh, device sync)
    """
    
    def __init__(self, config: Optional[LorentzConfig] = None):
        """
        Initialize Lorentz substrate.
        
        Args:
            config: LorentzConfig instance (uses defaults if None)
        """
        super().__init__()
        self.config = config or LorentzConfig()
        self.n = self.config.intrinsic_dim
        self.eps = self.config.eps

        val = math.log(self.config.initial_curvature)
        if self.config.learnable_curvature:
            self._log_K = nn.Parameter(torch.tensor(val))
        else:
            self.register_buffer('_log_K', torch.tensor(val))

    @property
    def K(self) -> torch.Tensor:
        """Returns curvature (K) ensuring correct device and autograd."""
        return torch.exp(self._log_K).clamp(
            min=self.config.curvature_min, 
            max=self.config.curvature_max
        )

    @property
    def curvature(self) -> torch.Tensor:
        """Alias for K for backward compatibility."""
        return self.K

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DIAGNOSTIC METHODS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def manifold_violation(self, x: torch.Tensor) -> torch.Tensor:
        """
        Computes mean error of âŸ¨x, xâŸ©_L + 1/K = 0.
        
        Should be â‰ˆ0 for valid points on manifold.
        """
        # Check for NaN/Inf in input
        if not torch.isfinite(x).all():
            return torch.tensor(float('nan'), device=x.device, dtype=x.dtype)
        
        K = self.K.to(x.device, x.dtype)
        inner = self.minkowski_inner(x, x).squeeze(-1)
        target = -1.0 / K
        
        # Handle NaN in computation
        violation = torch.abs(inner - target)
        violation = torch.nan_to_num(violation, nan=0.0, posinf=0.0, neginf=0.0)
        
        return violation.mean()

    def lorentz_radius(self, x: torch.Tensor) -> torch.Tensor:
        """
        Computes geodesic distance from point x to the hyperboloid origin.
        Essential for monitoring radial collapse in the Lorentz model.
        """
        K = self.K.to(x.device, x.dtype)
        x0 = x[..., 0]
        arg = (x0 * torch.sqrt(K)).clamp(min=1.0 + self.eps)
        return torch.acosh(arg) / torch.sqrt(K)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RIEMANNIAN OPTIMIZATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def riemannian_grad(self, x: torch.Tensor, grad_e: torch.Tensor) -> torch.Tensor:
        """
        Converts Euclidean gradient to Riemannian via tangent projection.
        
        Formula: g_r = g_e + K * âŸ¨x, g_eâŸ©_L * x
        
        Args:
            x: Point on manifold
            grad_e: Euclidean gradient
            
        Returns:
            Riemannian gradient in tangent space at x
        """
        inner = self.minkowski_inner(x, grad_e)
        K = self.K.to(x.device, x.dtype)
        return grad_e + K * inner * x

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CORE GEOMETRIC OPERATIONS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def minkowski_inner(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Lorentz inner product: âŸ¨x,yâŸ©_L = -xâ‚€yâ‚€ + xâ‚yâ‚ + ... + xâ‚™yâ‚™
        
        Returns tensor with keepdim=True for broadcasting compatibility.
        """
        return -x[..., :1] * y[..., :1] + (x[..., 1:] * y[..., 1:]).sum(dim=-1, keepdim=True)

    def proj(self, x: torch.Tensor) -> torch.Tensor:
        """
        Projects points onto the Lorentzian hyperboloid.
        
        Given spatial components x[1:], computes time component x[0]
        such that âŸ¨x,xâŸ©_L = -1/K.
        
        Formula: xâ‚€ = âˆš(1/K + ||x_s||Â²)
        """
        K = self.K.to(x.device, x.dtype)
        xs = x[..., 1:]
        x0 = torch.sqrt(1.0 / K + (xs ** 2).sum(dim=-1, keepdim=True) + self.eps)
        return torch.cat([x0, xs], dim=-1)

    def origin(self, batch_size: int = 1, device=None, dtype=None) -> torch.Tensor:
        """
        Generates origin point(s) synchronized with curvature device.
        
        Origin: (1/âˆšK, 0, 0, ..., 0)
        """
        if device is None:
            device = self._log_K.device
        if dtype is None:
            dtype = self._log_K.dtype
        K = self.K.to(device, dtype)
        o = torch.zeros(batch_size, self.n + 1, device=device, dtype=dtype)
        o[:, 0] = 1.0 / torch.sqrt(K)
        return o

    def exp_map(self, x: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        """
        Exponential Map (Retraction): T_x M â†’ M
        
        Maps tangent vector v at point x to the manifold.
        
        Formula:
            exp_x(v) = cosh(||v||_L * âˆšK) * x + sinh(||v||_L * âˆšK) * v/||v||_L
        """
        K = self.K.to(x.device, x.dtype)
        v_norm_sq = torch.abs(self.minkowski_inner(v, v)) + self.eps
        v_norm = torch.sqrt(v_norm_sq)
        scale = (v_norm * torch.sqrt(K)).clamp(max=15.0)
        cosh_scale = torch.cosh(scale)
        sinh_scale = torch.sinh(scale) / (v_norm + self.eps)
        return self.proj(cosh_scale * x + sinh_scale * v)

    def log_map(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Logarithmic Map: M â†’ T_x M
        
        Maps point y on manifold to tangent space at x.
        """
        K = self.K.to(x.device, x.dtype)
        inner = self.minkowski_inner(x, y)
        v = y + K * inner * x
        v_norm_sq = torch.abs(self.minkowski_inner(v, v)) + self.eps
        v_norm = torch.sqrt(v_norm_sq)
        d = self.dist(x, y).unsqueeze(-1)
        return d * v / (v_norm + self.eps)

    def log_map_zero(self, y: torch.Tensor) -> torch.Tensor:
        """Maps y to tangent space at origin."""
        o = self.origin(y.shape[0]).to(y.device, y.dtype)
        return self.log_map(o, y)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DISTANCE FUNCTIONS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def dist(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Hyperbolic geodesic distance between two points.
        
        Formula: d(x,y) = (1/âˆšK) * arccosh(-K * âŸ¨x,yâŸ©_L)
        """
        K = self.K.to(x.device, x.dtype)
        inner = self.minkowski_inner(x, y).squeeze(-1)
        arg = (-K * inner).clamp(min=1.0 + self.eps, max=1e5)
        dist = torch.acosh(arg) / torch.sqrt(K)
        # Replace NaN/Inf with zeros
        return torch.nan_to_num(dist, nan=0.0, posinf=0.0, neginf=0.0)

    def distance_matrix_points(
        self, 
        x: torch.Tensor, 
        y: torch.Tensor, 
        pairwise: bool = True
    ) -> torch.Tensor:
        """
        Computes GPU-optimized distance matrices.
        
        Used by HomeostaticField for anchor distances.
        
        Args:
            x: Points [N, dim+1]
            y: Points [M, dim+1]
            pairwise: If True, compute full NxM matrix
            
        Returns:
            Distance matrix [N, M] if pairwise, else [N]
        """
        K_val = self.K.to(x.device, x.dtype)
        if pairwise:
            spatial = torch.mm(x[:, 1:], y[:, 1:].t())
            time = torch.mm(x[:, :1], y[:, :1].t())
            inner = -time + spatial
        else:
            inner = self.minkowski_inner(x, y).squeeze(-1)

        arg = (-K_val * inner).clamp(min=1.0 + self.eps, max=1e5)
        dist = torch.acosh(arg) / (torch.sqrt(K_val) + 1e-9)
        
        # Replace NaN/Inf with zeros (numerical safety)
        dist = torch.nan_to_num(dist, nan=0.0, posinf=0.0, neginf=0.0)
        
        return dist

    def distance_matrix(self, x: torch.Tensor) -> torch.Tensor:
        """
        Generates pairwise geodesic distance matrix O(BÂ²).
        Compatible interface with PowerLawDistillation.
        """
        return self.distance_matrix_points(x, x, pairwise=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SIMILARITY FUNCTIONS [METRIC-CONSISTENT]
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def lorentz_similarity(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Lorentz similarity: 1 / (-K * âŸ¨x,yâŸ©_L)

        Returns similarity in (0, 1], where 1 = identical points.
        Monotonic transformation of Minkowski inner product.
        """
        K = self.K.to(x.device, x.dtype)
        inner = self.minkowski_inner(x, y).squeeze(-1)
        cosh_dist = (-K * inner).clamp(min=1.0 + self.eps)
        return 1.0 / cosh_dist

    def geodesic_similarity(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Geodesic similarity: exp(-d_H(x,y))

        METRIC-CONSISTENT with training that uses -D/Ï„ as logits.
        Returns similarity in (0, 1], where 1 = identical points.

        Properties:
        - Monotonically decreasing with distance
        - sim(x,x) = 1
        - Consistent with HyperbolicInfoNCE_Lorentz training objective
        """
        d = self.dist(x, y)
        return torch.exp(-d)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ALIASES FOR COMPATIBILITY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def project_to_manifold(self, x: torch.Tensor) -> torch.Tensor:
        """Alias for proj() for API compatibility."""
        return self.proj(x)
    
    def geodesic_distance(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """Alias for dist() for API compatibility."""
        return self.dist(x, y)
    
    def get_curvature(self) -> torch.Tensor:
        """Get current curvature value."""
        return self.K
    
    def to_poincare(self, x: torch.Tensor) -> torch.Tensor:
        """
        Convert from Lorentz (hyperboloid) to PoincarÃ© ball model.
        
        Maps point x = (x0, x1, ..., xn) on hyperboloid to
        point y = (x1, ..., xn) / (x0 + 1) in PoincarÃ© ball.
        
        Parameters
        ----------
        x : torch.Tensor
            Points on Lorentz manifold, shape (..., dim+1)
            
        Returns
        -------
        torch.Tensor
            Points in PoincarÃ© ball, shape (..., dim)
        """
        x0 = x[..., 0:1]  # Time component
        xi = x[..., 1:]   # Spatial components
        
        # Stereographic projection: y = xi / (x0 + 1)
        return xi / (x0 + 1.0 + 1e-8)
    
    def from_poincare(self, y: torch.Tensor) -> torch.Tensor:
        """
        Convert from PoincarÃ© ball to Lorentz (hyperboloid) model.
        
        Parameters
        ----------
        y : torch.Tensor
            Points in PoincarÃ© ball, shape (..., dim)
            
        Returns
        -------
        torch.Tensor
            Points on Lorentz manifold, shape (..., dim+1)
        """
        y_sqnorm = (y ** 2).sum(dim=-1, keepdim=True)
        
        # Inverse stereographic projection
        x0 = (1.0 + y_sqnorm) / (1.0 - y_sqnorm + 1e-8)
        xi = 2 * y / (1.0 - y_sqnorm + 1e-8)
        
        x = torch.cat([x0, xi], dim=-1)
        return self.proj(x)  # Ensure on manifold

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # LLM-OPTIMIZED OPERATIONS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def exp_map_zero(self, v: torch.Tensor) -> torch.Tensor:
        """
        Exponential map from origin (optimized for LLM embedding lookup).
        
        For tangent vectors at origin o = (1/âˆšK, 0, ..., 0):
            exp_o(v) = cosh(âˆšKÂ·||v||)Â·o + sinh(âˆšKÂ·||v||)Â·v/||v||
        
        Args:
            v: Tangent vectors at origin [..., n+1] where v[..., 0] = 0
            
        Returns:
            Points on manifold [..., n+1]
            
        Notes:
            - Input v must have v[..., 0] = 0 (valid tangent at origin)
            - Uses Taylor expansion for small norms (stability)
            - Clamps scale to prevent overflow in sinh/cosh
        """
        K = self.K.to(v.device, v.dtype)
        sqrt_K = torch.sqrt(K)
        
        # For tangent at origin: ||v||_L = ||v_spatial|| (Lorentz norm = Euclidean norm of spatial)
        v_spatial = v[..., 1:]
        
        # ðŸ”§ HARD NUMERICAL GUARD (CRÃTICO)
        v_spatial = torch.nan_to_num(
            v_spatial,
            nan=0.0,
            posinf=0.0,
            neginf=0.0,
        )
        
        v_norm = torch.norm(v_spatial, dim=-1, keepdim=True)
        
        # Protege domÃ­nio do exp / sinh / cosh
        v_norm = torch.nan_to_num(
            v_norm,
            nan=0.0,
            posinf=15.0,
            neginf=0.0,
        ).clamp(min=self.eps, max=15.0)
        
        scale = sqrt_K * v_norm
        
        # sinh/cosh computation
        cosh_scale = torch.cosh(scale)
        sinh_scale = torch.sinh(scale) / (v_norm + self.eps)
        
        # Origin point
        o_time = 1.0 / sqrt_K
        
        # exp_o(v) = cosh(...)Â·o + sinh(...)Â·v/||v||
        # Time component: cosh(...) * (1/âˆšK)
        # Spatial components: sinh(...) * v_spatial / ||v||
        x_time = cosh_scale * o_time
        x_spatial = sinh_scale * v_spatial
        
        return torch.cat([x_time, x_spatial], dim=-1)

    def log_map_zero_batch(self, y: torch.Tensor) -> torch.Tensor:
        """
        Logarithmic map to origin (batch-optimized for LLM).
        
        Maps points y on manifold to tangent space at origin.
        
        Args:
            y: Points on manifold [..., n+1]
            
        Returns:
            Tangent vectors at origin [..., n+1]
        """
        K = self.K.to(y.device, y.dtype)
        sqrt_K = torch.sqrt(K)
        
        # Distance from origin: d = arccosh(xâ‚€ Â· âˆšK) / âˆšK
        y0 = y[..., 0:1]
        arg = (y0 * sqrt_K).clamp(min=1.0 + self.eps, max=1e5)
        d = safe_acosh(arg) / sqrt_K
        
        # Direction: y_spatial / ||y_spatial||
        y_spatial = y[..., 1:]
        y_spatial_norm = torch.norm(y_spatial, dim=-1, keepdim=True).clamp(min=self.eps)
        
        # Tangent vector: d * direction (time component = 0)
        v_spatial = d * y_spatial / y_spatial_norm
        v_time = torch.zeros_like(y0)
        
        return torch.cat([v_time, v_spatial], dim=-1)

    def distance_matrix_batch(
        self, 
        q: torch.Tensor, 
        k: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute pairwise geodesic distances for attention (GPU-optimized).
        
        Args:
            q: Query points [B, L_q, n+1] or [B, H, L_q, n+1]
            k: Key points [B, L_k, n+1] or [B, H, L_k, n+1]
            
        Returns:
            Distance matrix [B, L_q, L_k] or [B, H, L_q, L_k]
        """
        K = self.K.to(q.device, q.dtype)
        
        # Minkowski inner product via batch matmul
        # âŸ¨q, kâŸ©_L = -qâ‚€kâ‚€ + qâ‚kâ‚ + ... + qâ‚™kâ‚™
        q_time = q[..., 0:1]  # [..., L_q, 1]
        q_space = q[..., 1:]  # [..., L_q, n]
        k_time = k[..., 0:1]  # [..., L_k, 1]
        k_space = k[..., 1:]  # [..., L_k, n]
        
        # Batch matrix multiply for spatial part
        space_inner = torch.matmul(q_space, k_space.transpose(-2, -1))  # [..., L_q, L_k]
        time_inner = torch.matmul(q_time, k_time.transpose(-2, -1))     # [..., L_q, L_k]
        
        inner = -time_inner + space_inner
        
        # Distance: d = (1/âˆšK) Â· arccosh(-K Â· âŸ¨q, kâŸ©_L)
        arg = (-K * inner).clamp(min=1.0 + self.eps, max=1e5)
        dist = safe_acosh(arg) / torch.sqrt(K)
        
        return torch.nan_to_num(dist, nan=0.0, posinf=0.0, neginf=0.0)

    def tangent_linear(
        self, 
        x: torch.Tensor, 
        weight: torch.Tensor, 
        bias: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Linear transformation in tangent space at origin.
        
        For LLM: log â†’ linear â†’ exp pattern.
        
        Args:
            x: Points on manifold [..., n+1]
            weight: Linear weight [out_dim, n] (operates on spatial only)
            bias: Optional bias [out_dim]
            
        Returns:
            Transformed points on manifold [..., out_dim+1]
        """
        # Map to tangent at origin
        v = self.log_map_zero_batch(x)
        v_spatial = v[..., 1:]  # [..., n]
        
        # Linear transformation (spatial only)
        out_spatial = F.linear(v_spatial, weight, bias)  # [..., out_dim]
        
        # Zero-pad time component for tangent vector
        out_time = torch.zeros(
            *out_spatial.shape[:-1], 1, 
            device=out_spatial.device, 
            dtype=out_spatial.dtype
        )
        v_out = torch.cat([out_time, out_spatial], dim=-1)
        
        # Map back to manifold
        return self.exp_map_zero(v_out)

    def weighted_midpoint(
        self, 
        x: torch.Tensor, 
        weights: torch.Tensor
    ) -> torch.Tensor:
        """
        Weighted FrÃ©chet mean approximation via tangent space aggregation.
        
        For attention value aggregation: maps to tangent, weighted sum, maps back.
        
        Args:
            x: Points on manifold [B, L, n+1]
            weights: Attention weights [B, L] (sum to 1 per row)
            
        Returns:
            Aggregated points [B, n+1]
            
        Notes:
            - This is an approximation to the true FrÃ©chet mean
            - Exact when all points are at origin
            - Error bounded by radius regularization
        """
        # Map to tangent at origin
        v = self.log_map_zero_batch(x)  # [B, L, n+1]
        
        # Weighted sum in tangent space
        weights = weights.unsqueeze(-1)  # [B, L, 1]
        v_mean = (v * weights).sum(dim=-2)  # [B, n+1]
        
        # Map back to manifold
        return self.exp_map_zero(v_mean)

    def attention_aggregate(
        self, 
        values: torch.Tensor, 
        attention_weights: torch.Tensor
    ) -> torch.Tensor:
        """
        Geodesic attention aggregation for transformer.
        
        Args:
            values: Value points [B, H, L, n+1]
            attention_weights: Softmax weights [B, H, L_q, L_k] (sum to 1 over L_k)
            
        Returns:
            Aggregated outputs [B, H, L_q, n+1]
        """
        # Map values to tangent space
        v = self.log_map_zero_batch(values)  # [B, H, L_k, n+1]
        
        # Weighted aggregation in tangent space
        # attention_weights: [B, H, L_q, L_k]
        # v: [B, H, L_k, n+1]
        # Result: [B, H, L_q, n+1]
        v_agg = torch.einsum('bhqk,bhkd->bhqd', attention_weights, v)
        
        # Map back to manifold
        return self.exp_map_zero(v_agg)

    def parallel_transport_zero(
        self, 
        v: torch.Tensor, 
        y: torch.Tensor
    ) -> torch.Tensor:
        """
        Parallel transport from origin to point y.
        
        Transports tangent vector v âˆˆ T_o M to T_y M.
        
        Args:
            v: Tangent vector at origin [..., n+1]
            y: Target point on manifold [..., n+1]
            
        Returns:
            Transported vector in T_y M [..., n+1]
        """
        K = self.K.to(v.device, v.dtype)
        
        # Inner products needed for transport
        inner_v_y = self.minkowski_inner(v, y)  # âŸ¨v, yâŸ©_L
        
        # Origin
        o = self.origin(1).to(v.device, v.dtype).squeeze(0)
        inner_o_y = self.minkowski_inner(o.unsqueeze(0), y)  # âŸ¨o, yâŸ©_L
        
        # Transport formula: v' = v - K Â· âŸ¨v, yâŸ©_L / (1 - K Â· âŸ¨o, yâŸ©_L) Â· (o + y)
        # Simplified for origin
        denom = 1.0 - K * inner_o_y + self.eps
        coeff = K * inner_v_y / denom
        
        return v - coeff * (o + y)
