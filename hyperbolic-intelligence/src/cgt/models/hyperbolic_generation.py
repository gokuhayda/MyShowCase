# GERA√á√ÉO HIPERB√ìLICA CORRETA - H-AKORN

import torch
import torch.nn.functional as F

class HLLMChatHyperbolic:
    """Chat com gera√ß√£o hiperb√≥lica correta."""
    
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.conversation = []
        self.model.eval()
        
        # Verificar se modelo tem substrate
        self.has_substrate = hasattr(model, 'substrate') and model.substrate is not None
        
        if self.has_substrate:
            print("‚úÖ Modelo com geometria hiperb√≥lica detectado")
            print("   Usando logits baseados em dist√¢ncia geod√©sica")
        else:
            print("‚ö†Ô∏è  Substrate n√£o encontrado")
            print("   Fallback para gera√ß√£o Euclidiana padr√£o")
    
    @torch.no_grad()
    def generate_hyperbolic(self, prompt, max_tokens=100, temperature=0.8, top_k=50, top_p=0.95):
        """
        Gera√ß√£o usando dist√¢ncias hiperb√≥licas (CORRETO segundo artigo).
        
        Specification 3.7: logit_i = -d_H(h_final, e_vocab_i) / œÑ
        """
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
        
        for step in range(max_tokens):
            # Forward pass
            outputs = self.model(input_ids)
            hidden_states = outputs.get('hidden_states')
            
            # Se temos hidden states e substrate, usar m√©todo hiperb√≥lico
            if hidden_states is not None and self.has_substrate:
                # √öltimo hidden state: [B, L, D] ou [B, L, D+1]
                h_final = hidden_states[:, -1, :]  # [B, D] ou [B, D+1]
                
                # Vocabul√°rio embeddings
                vocab_embeddings = self.model.embeddings.token_embeddings.weight  # [V, D]
                
                # Se dimens√µes n√£o batem, usar m√©todo Euclidiano
                if h_final.shape[-1] != vocab_embeddings.shape[-1]:
                    # Fallback: usar logits normais
                    logits = outputs['logits'][:, -1, :] / temperature
                else:
                    # M√âTODO CORRETO: Dist√¢ncias hiperb√≥licas
                    try:
                        # Expandir para broadcast: h_final [B, 1, D], vocab [1, V, D]
                        h_expanded = h_final.unsqueeze(1)  # [B, 1, D]
                        v_expanded = vocab_embeddings.unsqueeze(0)  # [1, V, D]
                        
                        # Calcular dist√¢ncias geod√©sicas
                        # d_H(h, e_i) para cada token i no vocabul√°rio
                        distances = self.model.substrate.dist(
                            h_expanded.expand(-1, vocab_embeddings.shape[0], -1).reshape(-1, h_final.shape[-1]),
                            v_expanded.expand(h_final.shape[0], -1, -1).reshape(-1, vocab_embeddings.shape[-1])
                        ).reshape(h_final.shape[0], vocab_embeddings.shape[0])
                        
                        # Logits = -dist√¢ncia / temperatura
                        logits = -distances / temperature
                        
                    except Exception as e:
                        # Se falhar, usar m√©todo Euclidiano
                        print(f"‚ö†Ô∏è  Hyperbolic generation failed: {e}")
                        logits = outputs['logits'][:, -1, :] / temperature
            else:
                # Sem hidden states ou substrate: m√©todo padr√£o
                logits = outputs['logits'][:, -1, :] / temperature
            
            # Top-k filtering (mesmo para ambos os m√©todos)
            if top_k > 0:
                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                logits[indices_to_remove] = float('-inf')
            
            # Top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits[indices_to_remove] = float('-inf')
            
            # Sample (Softmax ainda necess√°rio para normalizar probabilidades)
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            
            # Stop if EOS
            if next_token.item() == self.tokenizer.eos_token_id:
                break
        
        # Decode
        response = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        response = response[len(prompt):].strip()
        
        return response
    
    @torch.no_grad()
    def generate_euclidean(self, prompt, max_tokens=100, temperature=0.8, top_k=50, top_p=0.95):
        """
        Gera√ß√£o Euclidiana padr√£o (FALLBACK).
        """
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
        
        for _ in range(max_tokens):
            outputs = self.model(input_ids)
            logits = outputs['logits'][:, -1, :] / temperature
            
            if top_k > 0:
                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
                logits[indices_to_remove] = float('-inf')
            
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits[indices_to_remove] = float('-inf')
            
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            
            if next_token.item() == self.tokenizer.eos_token_id:
                break
        
        response = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        response = response[len(prompt):].strip()
        
        return response
    
    def generate(self, prompt, **kwargs):
        """
        Auto-select: Hyperbolic if available, Euclidean otherwise.
        """
        if self.has_substrate:
            return self.generate_hyperbolic(prompt, **kwargs)
        else:
            return self.generate_euclidean(prompt, **kwargs)


# COMPARA√á√ÉO: Euclidiano vs Hiperb√≥lico
def compare_generation_methods(model, tokenizer, device, prompt="Hello"):
    """
    Compara gera√ß√£o Euclidiana vs Hiperb√≥lica.
    """
    print("="*60)
    print("COMPARA√á√ÉO: Euclidiano vs Hiperb√≥lico")
    print("="*60)
    
    chat = HLLMChatHyperbolic(model, tokenizer, device)
    
    # M√©todo 1: Euclidiano (ATUAL - INCORRETO)
    print("\n1Ô∏è‚É£ M√âTODO EUCLIDIANO (Logits via nn.Linear)")
    print("-" * 60)
    response_euclidean = chat.generate_euclidean(prompt, max_tokens=50, temperature=0.8)
    print(f"Prompt: {prompt}")
    print(f"Response: {response_euclidean}")
    
    # M√©todo 2: Hiperb√≥lico (CORRETO segundo artigo)
    print("\n2Ô∏è‚É£ M√âTODO HIPERB√ìLICO (Logits via -d_H)")
    print("-" * 60)
    if chat.has_substrate:
        response_hyperbolic = chat.generate_hyperbolic(prompt, max_tokens=50, temperature=0.8)
        print(f"Prompt: {prompt}")
        print(f"Response: {response_hyperbolic}")
        
        print("\nüìä DIFEREN√áA:")
        print(f"   Euclidiano: {len(response_euclidean)} chars")
        print(f"   Hiperb√≥lico: {len(response_hyperbolic)} chars")
        
        if response_euclidean != response_hyperbolic:
            print("   ‚ö†Ô∏è  Respostas diferentes (esperado)")
        else:
            print("   ‚ö†Ô∏è  Respostas id√™nticas (suspeito - verificar implementa√ß√£o)")
    else:
        print("   ‚ö†Ô∏è  Substrate n√£o dispon√≠vel - n√£o √© poss√≠vel comparar")
    
    print("\n" + "="*60)
    
    return chat


# EXEMPLO DE USO
"""
# No Colab:
chat_hyperbolic = compare_generation_methods(model, tokenizer, device, "What is")

# Usar na interface
chat_hyperbolic.generate("Explain hyperbolic geometry", max_tokens=100)
"""
